{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "52YaS3rBs0X0"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5Nvo9n4ztCYX"
      },
      "outputs": [],
      "source": [
        "def create_sequences_and_targets(encoded_sentences_ids, max_sequence_length=30):\n",
        "    X_samples = []\n",
        "    y_samples = []\n",
        "    step_size = 1\n",
        "    for sentence_ids in encoded_sentences_ids:\n",
        "        if len(sentence_ids) < max_sequence_length + 1:\n",
        "            continue\n",
        "\n",
        "        # Use sliding window within each sentence\n",
        "        for i in range(0, len(sentence_ids) - max_sequence_length, step_size):\n",
        "            input_seq = sentence_ids[i:i+max_sequence_length]\n",
        "            target_seq = sentence_ids[i+1:i+max_sequence_length+1]\n",
        "\n",
        "            X_samples.append(input_seq)\n",
        "            y_samples.append(target_seq)\n",
        "\n",
        "    # Convert to tensors (no padding needed since all sequences are same length)\n",
        "    X = torch.tensor(X_samples, dtype=torch.long)\n",
        "    y = torch.tensor(y_samples, dtype=torch.long)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "class NextWordPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network for next word prediction with the following architecture:\n",
        "    Embedding layer -> LSTM layer -> Attention layer -> Fully connected layer\n",
        "\n",
        "    The model uses Luong (multiplicative) attention to focus on relevant parts\n",
        "    of the input sequence when making predictions.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embeddings_matrix, hidden_dim, num_layers, pad_token_id):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embeddings_matrix.shape[1]\n",
        "        self.embeddings_matrix = torch.from_numpy(embeddings_matrix).float()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        # Embedding layer: converts token IDs to dense vectors\n",
        "        self.embedding = nn.Embedding.from_pretrained(self.embeddings_matrix, freeze=False, padding_idx=self.pad_token_id)\n",
        "        # self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=self.pad_token_id)\n",
        "\n",
        "        # LSTM layer: processes sequential information\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
        "\n",
        "        # Attention layer: focuses on relevant parts of the sequence\n",
        "        self.attention = nn.Linear(self.hidden_dim, 1)\n",
        "\n",
        "        # Fully connected layer: maps attended features to vocabulary predictions\n",
        "        self.fc = nn.Linear(self.hidden_dim * 2, self.vocab_size)  # Modified input size to 2 * hidden_dim\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids, hidden=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Token IDs (batch_size, seq_len)\n",
        "            hidden: Initial hidden state for LSTM\n",
        "\n",
        "        Returns:\n",
        "            output: Logits for next word prediction (batch_size, seq_len, vocab_size)\n",
        "            hidden: Final hidden state from LSTM\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        # 1. Embedding layer\n",
        "        embedded = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # 2. LSTM layer\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)  # (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        # 3. Attention layer\n",
        "        # Simpler attention: just weight the LSTM outputs\n",
        "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "\n",
        "        # Weighted average of all positions for each timestep\n",
        "        attended_output = torch.bmm(attention_weights.transpose(1, 2), lstm_out)\n",
        "\n",
        "        # Use both attended and original LSTM output\n",
        "        combined = torch.cat([lstm_out, attended_output.expand_as(lstm_out)], dim=-1)\n",
        "\n",
        "        # 4. Fully connected layer\n",
        "        output = self.fc(combined)\n",
        "\n",
        "        # output = self.fc(context_vector)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        \"\"\"Initialize hidden state for LSTM.\"\"\"\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
        "\n",
        "def calculate_accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            hidden = model.init_hidden(batch_size, device)\n",
        "            output_logits, hidden = model(inputs, hidden)\n",
        "\n",
        "            predictions = torch.argmax(output_logits, dim=-1)  # (batch_size, seq_len)\n",
        "\n",
        "            correct_predictions += (predictions == targets).sum().item()\n",
        "            total_predictions += targets.numel()\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "def calculate_perplexity(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token_id, reduction='sum')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            hidden = model.init_hidden(batch_size, device)\n",
        "            output_logits, hidden = model(inputs, hidden)\n",
        "\n",
        "            loss = criterion(output_logits.view(-1, model.vocab_size), targets.view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += targets.numel()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, num_epochs, vocab_size, device=\"cpu\", clip_grad_norm=1.0):\n",
        "    model.train()\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.00001)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dataloader), epochs=num_epochs, pct_start=0.1)\n",
        "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token_id)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "            hidden = model.init_hidden(batch_size, device)\n",
        "            optimizer.zero_grad()\n",
        "            output_logits, hidden = model(inputs, hidden)\n",
        "\n",
        "            hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "            loss = criterion(output_logits.view(-1, vocab_size), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for inputs, targets in val_dataloader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                batch_size = inputs.size(0)\n",
        "                val_hidden = model.init_hidden(batch_size, device)\n",
        "                output_logits, val_hidden = model(inputs, val_hidden)\n",
        "                val_hidden = (val_hidden[0].detach(), val_hidden[1].detach())\n",
        "\n",
        "                loss = criterion(output_logits.view(-1, vocab_size), targets.view(-1))\n",
        "                val_loss += loss.item()\n",
        "        avg_val_loss = val_loss / len(val_dataloader)\n",
        "        scheduler.step() # Removed epoch argument\n",
        "\n",
        "        # Calculate accuracy and perplexity every 5 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(\"Calculating metrics...\")\n",
        "            train_accuracy = calculate_accuracy(model, train_dataloader, device)\n",
        "            val_accuracy = calculate_accuracy(model, val_dataloader, device)\n",
        "\n",
        "            train_perplexity = calculate_perplexity(model, train_dataloader, device)\n",
        "            val_perplexity = calculate_perplexity(model, val_dataloader, device)\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n",
        "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "            print(f\"  Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "            print(f\"  Train Perplexity: {train_perplexity:.2f}, Val Perplexity: {val_perplexity:.2f}\")\n",
        "            print(\"-\" * 60)\n",
        "        else:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "def evaluate_model(model, test_dataloader, device):\n",
        "    print(\"Evaluating model on test set...\")\n",
        "\n",
        "    test_accuracy = calculate_accuracy(model, test_dataloader, device)\n",
        "    test_perplexity = calculate_perplexity(model, test_dataloader, device)\n",
        "\n",
        "    print(f\"Final Test Results:\")\n",
        "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"  Test Perplexity: {test_perplexity:.2f}\")\n",
        "\n",
        "    # Check if metrics meet assignment requirements\n",
        "    print(\"\\nAssignment Requirements Check:\")\n",
        "    print(f\"  Test Accuracy > 75%: {'Pass' if test_accuracy > 0.75 else 'Fail'} ({test_accuracy:.1%})\")\n",
        "    print(f\"  Perplexity < 250: {'Pass' if test_perplexity < 250 else 'Fail'} ({test_perplexity:.1f})\")\n",
        "\n",
        "    return test_accuracy, test_perplexity\n",
        "\n",
        "def text_to_indices(text, word_to_idx, max_sequence_length=30):\n",
        "    words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text.lower())\n",
        "    indices = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in word_to_idx:\n",
        "            indices.append(word_to_idx[word])\n",
        "        else:\n",
        "            indices.append(word_to_idx['<UNK>'])\n",
        "\n",
        "    if max_sequence_length and len(indices) > max_sequence_length:\n",
        "        indices = indices[-max_sequence_length:]\n",
        "\n",
        "    return indices\n",
        "\n",
        "def indices_to_text(indices, idx_to_word):\n",
        "    words = []\n",
        "    special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']\n",
        "    for idx in indices:\n",
        "        if idx in idx_to_word:\n",
        "            if idx_to_word[idx] in special_tokens:\n",
        "                continue\n",
        "            words.append(idx_to_word[idx])\n",
        "    content = ''.join(words)\n",
        "    content = re.sub(r'\\s+', ' ', content).strip()\n",
        "    return content\n",
        "\n",
        "def generate_text(model, word_to_idx, idx_to_word, start_text, num_words_to_generate, top5= False, max_sequence_length=30, device=\"cpu\", temperature=1.0):\n",
        "\n",
        "    model.eval()\n",
        "    generated_ids = []\n",
        "    prediction_details = []\n",
        "\n",
        "    # Encode the start text\n",
        "    current_input_ids = text_to_indices(start_text, word_to_idx, max_sequence_length)\n",
        "\n",
        "    generated_ids.extend(current_input_ids)\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    input_tensor = torch.tensor([current_input_ids], dtype=torch.long).to(device)\n",
        "    hidden = model.init_hidden(1, device)\n",
        "    print(f\"Generating from: '{start_text}'\")\n",
        "\n",
        "    for i in range(num_words_to_generate):\n",
        "        with torch.no_grad():\n",
        "            output_logits, hidden = model(input_tensor, hidden)\n",
        "            last_word_logits = output_logits[0, -1, :]  # (vocab_size,)\n",
        "            probabilities = torch.softmax(last_word_logits / temperature, dim=-1)\n",
        "\n",
        "            top_5_probs, top_5_indices = torch.topk(probabilities, 5)\n",
        "            top_5_words_and_probs = []\n",
        "            for j in range(len(top_5_indices)):\n",
        "                word = idx_to_word.get(top_5_indices[j].item(), '<UNK>')\n",
        "                prob = top_5_probs[j].item()\n",
        "                top_5_words_and_probs.append(f\"{word} ({prob:.4f})\")\n",
        "\n",
        "            predicted_id = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            prediction_details.append({\n",
        "                \"step\": i + 1,\n",
        "                \"top_5_alternatives\": top_5_words_and_probs\n",
        "            })\n",
        "            generated_ids.append(predicted_id)\n",
        "\n",
        "            current_input_ids.append(predicted_id)\n",
        "\n",
        "            if len(current_input_ids) > max_sequence_length:\n",
        "                current_input_ids = current_input_ids[1:]\n",
        "\n",
        "            input_tensor = torch.tensor([current_input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    generated_text = indices_to_text(generated_ids, idx_to_word)\n",
        "    if top5 == True:\n",
        "      print(\"\\n--- Prediction Details ---\")\n",
        "      for detail in prediction_details:\n",
        "          print(f\"Step {detail['step']}:'\")\n",
        "          print(f\"  Top 5 likely words: {', '.join(detail['top_5_alternatives'])}\")\n",
        "      print(\"--------------------------\\n\")\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QVzMNPxtHxU",
        "outputId": "7e5a8de5-8c9c-4fab-de61-48d15a016f2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 4389\n"
          ]
        }
      ],
      "source": [
        "with open(\"embeddings.pkl\", 'rb') as f:\n",
        "    embeddings = pickle.load(f)\n",
        "with open(\"data.pkl\", 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "train_sequences = data['train_sequences']\n",
        "val_sequences = data['val_sequences']\n",
        "test_sequences = data['test_sequences']\n",
        "\n",
        "word_to_idx = embeddings['word_to_idx']\n",
        "idx_to_word = embeddings['idx_to_word']\n",
        "embeddings_matrix = embeddings['embeddings_matrix']\n",
        "\n",
        "pad_token_id = word_to_idx[\"<PAD>\"]\n",
        "vocab_size = len(word_to_idx)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "embedding_dim = embeddings_matrix.shape[1]\n",
        "batch_size = 128\n",
        "\n",
        "X_train, y_train = create_sequences_and_targets(train_sequences)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "X_val, y_val = create_sequences_and_targets(val_sequences)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "X_test, y_test = create_sequences_and_targets(test_sequences)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw1W_AXFtsin",
        "outputId": "7544462f-26e0-4371-9076-428ee38ec90f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlyik2rdx1Uq",
        "outputId": "fa80be27-e0fd-4c33-c8dd-04e92e0365d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "191178"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxWRkIIHtuzy",
        "outputId": "ad966544-96c4-4646-f404-20008d16cc84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NextWordPredictor(\n",
              "  (embedding): Embedding(4389, 384, padding_idx=0)\n",
              "  (lstm): LSTM(384, 256, num_layers=2, batch_first=True)\n",
              "  (attention): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (fc): Linear(in_features=512, out_features=4389, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = NextWordPredictor(vocab_size, embeddings_matrix, hidden_dim=256, num_layers=2, pad_token_id=pad_token_id)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsiEfiU2uuS4",
        "outputId": "683894ac-c304-4115-f645-e7dd3261d143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch [1/100] Train Loss: 4.1094, Validation Loss: 3.3454\n",
            "Epoch [2/100] Train Loss: 3.1216, Validation Loss: 3.0663\n",
            "Epoch [3/100] Train Loss: 2.9535, Validation Loss: 2.9205\n",
            "Epoch [4/100] Train Loss: 2.8096, Validation Loss: 2.8097\n",
            "Epoch [5/100] Train Loss: 2.7057, Validation Loss: 2.7367\n",
            "Epoch [6/100] Train Loss: 2.6247, Validation Loss: 2.6690\n",
            "Epoch [7/100] Train Loss: 2.5422, Validation Loss: 2.5997\n",
            "Epoch [8/100] Train Loss: 2.4585, Validation Loss: 2.5366\n",
            "Epoch [9/100] Train Loss: 2.3821, Validation Loss: 2.4822\n",
            "Calculating metrics...\n",
            "Epoch [10/100]:\n",
            "  Train Loss: 2.3118, Val Loss: 2.4302\n",
            "  Train Accuracy: 0.5871, Val Accuracy: 0.5791\n",
            "  Train Perplexity: 9.65, Val Perplexity: 11.36\n",
            "------------------------------------------------------------\n",
            "Epoch [11/100] Train Loss: 2.2434, Validation Loss: 2.3824\n",
            "Epoch [12/100] Train Loss: 2.1790, Validation Loss: 2.3385\n",
            "Epoch [13/100] Train Loss: 2.1191, Validation Loss: 2.2996\n",
            "Epoch [14/100] Train Loss: 2.0634, Validation Loss: 2.2647\n",
            "Epoch [15/100] Train Loss: 2.0106, Validation Loss: 2.2314\n",
            "Epoch [16/100] Train Loss: 1.9611, Validation Loss: 2.2014\n",
            "Epoch [17/100] Train Loss: 1.9145, Validation Loss: 2.1750\n",
            "Epoch [18/100] Train Loss: 1.8704, Validation Loss: 2.1487\n",
            "Epoch [19/100] Train Loss: 1.8280, Validation Loss: 2.1243\n",
            "Calculating metrics...\n",
            "Epoch [20/100]:\n",
            "  Train Loss: 1.7869, Val Loss: 2.1016\n",
            "  Train Accuracy: 0.6558, Val Accuracy: 0.6294\n",
            "  Train Perplexity: 5.74, Val Perplexity: 8.18\n",
            "------------------------------------------------------------\n",
            "Epoch [21/100] Train Loss: 1.7476, Validation Loss: 2.0803\n",
            "Epoch [22/100] Train Loss: 1.7093, Validation Loss: 2.0593\n",
            "Epoch [23/100] Train Loss: 1.6721, Validation Loss: 2.0400\n",
            "Epoch [24/100] Train Loss: 1.6357, Validation Loss: 2.0203\n",
            "Epoch [25/100] Train Loss: 1.6006, Validation Loss: 2.0008\n",
            "Epoch [26/100] Train Loss: 1.5661, Validation Loss: 1.9838\n",
            "Epoch [27/100] Train Loss: 1.5330, Validation Loss: 1.9654\n",
            "Epoch [28/100] Train Loss: 1.5004, Validation Loss: 1.9476\n",
            "Epoch [29/100] Train Loss: 1.4690, Validation Loss: 1.9314\n",
            "Calculating metrics...\n",
            "Epoch [30/100]:\n",
            "  Train Loss: 1.4382, Val Loss: 1.9165\n",
            "  Train Accuracy: 0.7125, Val Accuracy: 0.6604\n",
            "  Train Perplexity: 4.04, Val Perplexity: 6.79\n",
            "------------------------------------------------------------\n",
            "Epoch [31/100] Train Loss: 1.4082, Validation Loss: 1.9003\n",
            "Epoch [32/100] Train Loss: 1.3791, Validation Loss: 1.8854\n",
            "Epoch [33/100] Train Loss: 1.3508, Validation Loss: 1.8697\n",
            "Epoch [34/100] Train Loss: 1.3227, Validation Loss: 1.8557\n",
            "Epoch [35/100] Train Loss: 1.2954, Validation Loss: 1.8405\n",
            "Epoch [36/100] Train Loss: 1.2688, Validation Loss: 1.8261\n",
            "Epoch [37/100] Train Loss: 1.2424, Validation Loss: 1.8122\n",
            "Epoch [38/100] Train Loss: 1.2166, Validation Loss: 1.7977\n",
            "Epoch [39/100] Train Loss: 1.1915, Validation Loss: 1.7853\n",
            "Calculating metrics...\n",
            "Epoch [40/100]:\n",
            "  Train Loss: 1.1671, Val Loss: 1.7710\n",
            "  Train Accuracy: 0.7637, Val Accuracy: 0.6853\n",
            "  Train Perplexity: 3.06, Val Perplexity: 5.87\n",
            "------------------------------------------------------------\n",
            "Epoch [41/100] Train Loss: 1.1427, Validation Loss: 1.7581\n",
            "Epoch [42/100] Train Loss: 1.1191, Validation Loss: 1.7461\n",
            "Epoch [43/100] Train Loss: 1.0964, Validation Loss: 1.7328\n",
            "Epoch [44/100] Train Loss: 1.0737, Validation Loss: 1.7209\n",
            "Epoch [45/100] Train Loss: 1.0515, Validation Loss: 1.7084\n",
            "Epoch [46/100] Train Loss: 1.0297, Validation Loss: 1.6973\n",
            "Epoch [47/100] Train Loss: 1.0089, Validation Loss: 1.6860\n",
            "Epoch [48/100] Train Loss: 0.9879, Validation Loss: 1.6737\n",
            "Epoch [49/100] Train Loss: 0.9676, Validation Loss: 1.6628\n",
            "Calculating metrics...\n",
            "Epoch [50/100]:\n",
            "  Train Loss: 0.9477, Val Loss: 1.6541\n",
            "  Train Accuracy: 0.8075, Val Accuracy: 0.7070\n",
            "  Train Perplexity: 2.45, Val Perplexity: 5.22\n",
            "------------------------------------------------------------\n",
            "Epoch [51/100] Train Loss: 0.9286, Validation Loss: 1.6413\n",
            "Epoch [52/100] Train Loss: 0.9096, Validation Loss: 1.6316\n",
            "Epoch [53/100] Train Loss: 0.8911, Validation Loss: 1.6217\n",
            "Epoch [54/100] Train Loss: 0.8730, Validation Loss: 1.6116\n",
            "Epoch [55/100] Train Loss: 0.8550, Validation Loss: 1.6031\n",
            "Epoch [56/100] Train Loss: 0.8380, Validation Loss: 1.5927\n",
            "Epoch [57/100] Train Loss: 0.8210, Validation Loss: 1.5836\n",
            "Epoch [58/100] Train Loss: 0.8045, Validation Loss: 1.5746\n",
            "Epoch [59/100] Train Loss: 0.7885, Validation Loss: 1.5668\n",
            "Calculating metrics...\n",
            "Epoch [60/100]:\n",
            "  Train Loss: 0.7727, Val Loss: 1.5569\n",
            "  Train Accuracy: 0.8456, Val Accuracy: 0.7268\n",
            "  Train Perplexity: 2.05, Val Perplexity: 4.74\n",
            "------------------------------------------------------------\n",
            "Epoch [61/100] Train Loss: 0.7573, Validation Loss: 1.5498\n",
            "Epoch [62/100] Train Loss: 0.7422, Validation Loss: 1.5417\n",
            "Epoch [63/100] Train Loss: 0.7276, Validation Loss: 1.5324\n",
            "Epoch [64/100] Train Loss: 0.7133, Validation Loss: 1.5255\n",
            "Epoch [65/100] Train Loss: 0.6994, Validation Loss: 1.5173\n",
            "Epoch [66/100] Train Loss: 0.6857, Validation Loss: 1.5093\n",
            "Epoch [67/100] Train Loss: 0.6722, Validation Loss: 1.5015\n",
            "Epoch [68/100] Train Loss: 0.6594, Validation Loss: 1.4935\n",
            "Epoch [69/100] Train Loss: 0.6465, Validation Loss: 1.4877\n",
            "Calculating metrics...\n",
            "Epoch [70/100]:\n",
            "  Train Loss: 0.6341, Val Loss: 1.4798\n",
            "  Train Accuracy: 0.8768, Val Accuracy: 0.7427\n",
            "  Train Perplexity: 1.78, Val Perplexity: 4.39\n",
            "------------------------------------------------------------\n",
            "Epoch [71/100] Train Loss: 0.6217, Validation Loss: 1.4738\n",
            "Epoch [72/100] Train Loss: 0.6099, Validation Loss: 1.4683\n",
            "Epoch [73/100] Train Loss: 0.5984, Validation Loss: 1.4598\n",
            "Epoch [74/100] Train Loss: 0.5871, Validation Loss: 1.4548\n",
            "Epoch [75/100] Train Loss: 0.5759, Validation Loss: 1.4485\n",
            "Epoch [76/100] Train Loss: 0.5651, Validation Loss: 1.4422\n",
            "Epoch [77/100] Train Loss: 0.5541, Validation Loss: 1.4360\n",
            "Epoch [78/100] Train Loss: 0.5440, Validation Loss: 1.4306\n",
            "Epoch [79/100] Train Loss: 0.5342, Validation Loss: 1.4238\n",
            "Calculating metrics...\n",
            "Epoch [80/100]:\n",
            "  Train Loss: 0.5242, Val Loss: 1.4202\n",
            "  Train Accuracy: 0.9012, Val Accuracy: 0.7566\n",
            "  Train Perplexity: 1.60, Val Perplexity: 4.13\n",
            "------------------------------------------------------------\n",
            "Epoch [81/100] Train Loss: 0.5145, Validation Loss: 1.4137\n",
            "Epoch [82/100] Train Loss: 0.5050, Validation Loss: 1.4064\n",
            "Epoch [83/100] Train Loss: 0.4960, Validation Loss: 1.4021\n",
            "Epoch [84/100] Train Loss: 0.4872, Validation Loss: 1.3964\n",
            "Epoch [85/100] Train Loss: 0.4782, Validation Loss: 1.3928\n",
            "Epoch [86/100] Train Loss: 0.4698, Validation Loss: 1.3870\n",
            "Epoch [87/100] Train Loss: 0.4614, Validation Loss: 1.3824\n",
            "Epoch [88/100] Train Loss: 0.4531, Validation Loss: 1.3784\n",
            "Epoch [89/100] Train Loss: 0.4451, Validation Loss: 1.3726\n",
            "Calculating metrics...\n",
            "Epoch [90/100]:\n",
            "  Train Loss: 0.4371, Val Loss: 1.3686\n",
            "  Train Accuracy: 0.9209, Val Accuracy: 0.7687\n",
            "  Train Perplexity: 1.46, Val Perplexity: 3.92\n",
            "------------------------------------------------------------\n",
            "Epoch [91/100] Train Loss: 0.4295, Validation Loss: 1.3640\n",
            "Epoch [92/100] Train Loss: 0.4220, Validation Loss: 1.3594\n",
            "Epoch [93/100] Train Loss: 0.4146, Validation Loss: 1.3547\n",
            "Epoch [94/100] Train Loss: 0.4075, Validation Loss: 1.3504\n",
            "Epoch [95/100] Train Loss: 0.4007, Validation Loss: 1.3467\n",
            "Epoch [96/100] Train Loss: 0.3937, Validation Loss: 1.3436\n",
            "Epoch [97/100] Train Loss: 0.3871, Validation Loss: 1.3404\n",
            "Epoch [98/100] Train Loss: 0.3805, Validation Loss: 1.3368\n",
            "Epoch [99/100] Train Loss: 0.3744, Validation Loss: 1.3308\n",
            "Calculating metrics...\n",
            "Epoch [100/100]:\n",
            "  Train Loss: 0.3679, Val Loss: 1.3294\n",
            "  Train Accuracy: 0.9364, Val Accuracy: 0.7790\n",
            "  Train Perplexity: 1.37, Val Perplexity: 3.77\n",
            "------------------------------------------------------------\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "train_model(model, train_dataloader, val_dataloader, num_epochs=100, vocab_size=vocab_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeGiZ-oUxA-t",
        "outputId": "9f670370-f71d-4e3f-bef1-b89f12768687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating from: 'He never spoke of the'\n",
            "\n",
            "--- Prediction Details ---\n",
            "Step 1:'\n",
            "  Top 5 likely words:   (1.0000), - (0.0000), , (0.0000), ? (0.0000), ! (0.0000)\n",
            "Step 2:'\n",
            "  Top 5 likely words: temple (0.6354), room (0.1881), house (0.0449), men (0.0416), vacancy (0.0189)\n",
            "Step 3:'\n",
            "  Top 5 likely words:   (1.0000), . (0.0000), ? (0.0000), - (0.0000), , (0.0000)\n",
            "Step 4:'\n",
            "  Top 5 likely words: to (0.9332), for (0.0173), never (0.0150), in (0.0092), as (0.0060)\n",
            "Step 5:'\n",
            "  Top 5 likely words:   (0.9987), - (0.0013), ? (0.0000), . (0.0000), , (0.0000)\n",
            "Step 6:'\n",
            "  Top 5 likely words: see (0.3064), be (0.1573), leave (0.0850), find (0.0841), do (0.0615)\n",
            "Step 7:'\n",
            "  Top 5 likely words:   (0.9972), - (0.0011), ? (0.0008), . (0.0005), , (0.0004)\n",
            "Step 8:'\n",
            "  Top 5 likely words: their (0.4301), his (0.1482), such (0.1245), your (0.0876), every (0.0291)\n",
            "Step 9:'\n",
            "  Top 5 likely words:   (0.9998), - (0.0002), ? (0.0000), , (0.0000), ! (0.0000)\n",
            "Step 10:'\n",
            "  Top 5 likely words: long (0.6300), leave (0.1240), simple (0.0311), room (0.0226), cases (0.0203)\n",
            "--------------------------\n",
            "\n",
            "\n",
            "--- Generated Text ---\n",
            "he never spoke of the temple to spare his leave\n"
          ]
        }
      ],
      "source": [
        "start_seed_text = \"He never spoke of the\"\n",
        "num_words = 10\n",
        "generated_output = generate_text(model, word_to_idx, idx_to_word, start_seed_text, num_words, top5=True, temperature=0.7, device=device)\n",
        "\n",
        "print(\"\\n--- Generated Text ---\")\n",
        "print(generated_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFtDmS4a2FBW",
        "outputId": "5c29420f-e6fe-4fdb-e2c9-183050ad39ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating from: 'Boots which extended halfway'\n",
            "\n",
            "--- Generated Text ---\n",
            "boots which extended halfway up the river in which which you found in which you earn in the opium den, which is unable to which in which\n"
          ]
        }
      ],
      "source": [
        "start_seed_text = \"Boots which extended halfway\"\n",
        "num_words = 50\n",
        "generated_output = generate_text(model, word_to_idx, idx_to_word, start_seed_text, num_words, temperature=0.7, device=device)\n",
        "\n",
        "print(\"\\n--- Generated Text ---\")\n",
        "print(generated_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn6MBGSbGbkx",
        "outputId": "c87bbb2d-4405-4943-f70b-1e286d13455f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating from: 'I ordered him to pay'\n",
            "\n",
            "--- Generated Text ---\n",
            "i ordered him to pay me and i have got back on i to come back on him on a and passed on either side on either\n"
          ]
        }
      ],
      "source": [
        "start_seed_text = \"I ordered him to pay\"\n",
        "num_words = 50\n",
        "generated_output = generate_text(model, word_to_idx, idx_to_word, start_seed_text, num_words, temperature=0.7, device=device)\n",
        "\n",
        "print(\"\\n--- Generated Text ---\")\n",
        "print(generated_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgxYWZWPGtLl",
        "outputId": "60b9f6d2-2325-4fc1-a91a-1fee6b858951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating from: 'I answered that it had'\n",
            "\n",
            "--- Generated Text ---\n",
            "i answered that it had been watching it, that it was that there should be safe, it all that it was there, that it must be , well\n"
          ]
        }
      ],
      "source": [
        "start_seed_text = \"I answered that it had\"\n",
        "num_words = 50\n",
        "generated_output = generate_text(model, word_to_idx, idx_to_word, start_seed_text, num_words, temperature=0.7, device=device)\n",
        "\n",
        "print(\"\\n--- Generated Text ---\")\n",
        "print(generated_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDiT-eQKG5_N",
        "outputId": "b3ca5258-39b8-4004-c0a3-61c85a53a89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbaHUeU2G-l8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
