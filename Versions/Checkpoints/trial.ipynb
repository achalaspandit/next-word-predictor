{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downloader import download_sherlock_holmes_text\n",
    "from data_processor import prepare_data_and_embeddings\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b67db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_and_targets(encoded_sentences_ids, max_sequence_length=30):\n",
    "    X_samples = []\n",
    "    y_samples = []\n",
    "    step_size = 10\n",
    "    for sentence_ids in encoded_sentences_ids:\n",
    "        if len(sentence_ids) < max_sequence_length + 1:\n",
    "            continue\n",
    "            \n",
    "        # Use sliding window within each sentence\n",
    "        for i in range(len(sentence_ids) - max_sequence_length):\n",
    "            input_seq = sentence_ids[i:i+max_sequence_length]\n",
    "            target_seq = sentence_ids[i+1:i+max_sequence_length+1]\n",
    "            \n",
    "            X_samples.append(input_seq)\n",
    "            y_samples.append(target_seq)\n",
    "    \n",
    "    # Convert to tensors (no padding needed since all sequences are same length)\n",
    "    X = torch.tensor(X_samples, dtype=torch.long)\n",
    "    y = torch.tensor(y_samples, dtype=torch.long)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "class NextWordPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for next word prediction with the following architecture:\n",
    "    Embedding layer -> LSTM layer -> Attention layer -> Fully connected layer\n",
    "    \n",
    "    The model uses Luong (multiplicative) attention to focus on relevant parts\n",
    "    of the input sequence when making predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embeddings_matrix, hidden_dim, num_layers, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embeddings_matrix.shape[1]\n",
    "        self.embeddings_matrix = torch.from_numpy(embeddings_matrix).float()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.pad_token_id = pad_token_id\n",
    "        \n",
    "        # Embedding layer: converts token IDs to dense vectors\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.embeddings_matrix, freeze=False, padding_idx=self.pad_token_id)\n",
    "        # self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=self.pad_token_id)\n",
    "        \n",
    "        # LSTM layer: processes sequential information\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        \n",
    "        # Attention layer: focuses on relevant parts of the sequence\n",
    "        self.attention = nn.Linear(self.hidden_dim, 1)\n",
    "        \n",
    "        # Fully connected layer: maps attended features to vocabulary predictions\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.vocab_size)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len)\n",
    "            hidden: Initial hidden state for LSTM\n",
    "            \n",
    "        Returns:\n",
    "            output: Logits for next word prediction (batch_size, seq_len, vocab_size)\n",
    "            hidden: Final hidden state from LSTM\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        # 1. Embedding layer\n",
    "        embedded = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # 2. LSTM layer\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 3. Attention layer\n",
    "        # Calculate attention weights for each position in the sequence\n",
    "        attention_scores = self.attention(lstm_out)  # (batch_size, seq_len, 1)\n",
    "        attention_weights = torch.softmax(attention_scores.squeeze(-1), dim=-1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Apply attention to create context vectors for each position\n",
    "        # For next word prediction, we need to compute context for each position\n",
    "        context_vectors = []\n",
    "        for i in range(seq_len):\n",
    "            # For position i, we can only attend to positions 0 to i (causal attention)\n",
    "            causal_weights = attention_weights[:, :i+1]  # (batch_size, i+1)\n",
    "            causal_weights = causal_weights / causal_weights.sum(dim=1, keepdim=True)  # Renormalize\n",
    "            \n",
    "            # Compute weighted sum of LSTM outputs up to position i\n",
    "            context = torch.bmm(causal_weights.unsqueeze(1), lstm_out[:, :i+1, :])  # (batch_size, 1, hidden_dim)\n",
    "            context_vectors.append(context.squeeze(1))  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Stack context vectors for all positions\n",
    "        context_vector = torch.stack(context_vectors, dim=1)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 4. Fully connected layer\n",
    "        output = self.fc(context_vector)  # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"Initialize hidden state for LSTM.\"\"\"\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
    "\n",
    "def calculate_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            \n",
    "            hidden = model.init_hidden(batch_size, device)\n",
    "            output_logits, hidden = model(inputs, hidden)\n",
    "            \n",
    "            predictions = torch.argmax(output_logits, dim=-1)  # (batch_size, seq_len)\n",
    "            \n",
    "            correct_predictions += (predictions == targets).sum().item()\n",
    "            total_predictions += targets.numel() \n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy \n",
    "\n",
    "def calculate_perplexity(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token_id, reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            \n",
    "            hidden = model.init_hidden(batch_size, device)\n",
    "            output_logits, hidden = model(inputs, hidden)\n",
    "            \n",
    "            loss = criterion(output_logits.view(-1, model.vocab_size), targets.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += targets.numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, num_epochs, vocab_size, device=\"cpu\", clip_grad_norm=1.0):\n",
    "    model.train()\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token_id)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            hidden = model.init_hidden(batch_size, device)\n",
    "            optimizer.zero_grad()\n",
    "            output_logits, hidden = model(inputs, hidden)\n",
    "\n",
    "            hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "            loss = criterion(output_logits.view(-1, vocab_size), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for inputs, targets in val_dataloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                batch_size = inputs.size(0)\n",
    "                val_hidden = model.init_hidden(batch_size, device)\n",
    "                output_logits, val_hidden = model(inputs, val_hidden)\n",
    "                val_hidden = (val_hidden[0].detach(), val_hidden[1].detach())\n",
    "\n",
    "                loss = criterion(output_logits.view(-1, vocab_size), targets.view(-1))\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Calculate accuracy and perplexity every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(\"Calculating metrics...\")\n",
    "            train_accuracy = calculate_accuracy(model, train_dataloader, device)\n",
    "            val_accuracy = calculate_accuracy(model, val_dataloader, device)\n",
    "            \n",
    "            train_perplexity = calculate_perplexity(model, train_dataloader, device)\n",
    "            val_perplexity = calculate_perplexity(model, val_dataloader, device)\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"  Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"  Train Perplexity: {train_perplexity:.2f}, Val Perplexity: {val_perplexity:.2f}\")\n",
    "            print(\"-\" * 60)\n",
    "        else:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "def evaluate_model(model, test_dataloader, device):\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    \n",
    "    test_accuracy = calculate_accuracy(model, test_dataloader, device)\n",
    "    test_perplexity = calculate_perplexity(model, test_dataloader, device)\n",
    "    \n",
    "    print(f\"Final Test Results:\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Test Perplexity: {test_perplexity:.2f}\")\n",
    "    \n",
    "    # Check if metrics meet assignment requirements\n",
    "    print(\"\\nAssignment Requirements Check:\")\n",
    "    print(f\"  Test Accuracy > 75%: {'✓' if test_accuracy > 0.75 else '✗'} ({test_accuracy:.1%})\")\n",
    "    print(f\"  Perplexity < 250: {'✓' if test_perplexity < 250 else '✗'} ({test_perplexity:.1f})\")\n",
    "    \n",
    "    return test_accuracy, test_perplexity\n",
    "\n",
    "def text_to_indices(text, word_to_idx, max_sequence_length=30):\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "    indices = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            indices.append(word_to_idx[word])\n",
    "        else:\n",
    "            indices.append(word_to_idx['<UNK>'])\n",
    "    \n",
    "    if max_sequence_length and len(indices) > max_sequence_length:\n",
    "        indices = indices[-max_sequence_length:]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def indices_to_text(indices, idx_to_word):\n",
    "    words = []\n",
    "    special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']\n",
    "    for idx in indices:\n",
    "        if idx in special_tokens:\n",
    "            continue\n",
    "        if idx in idx_to_word:\n",
    "            words.append(idx_to_word[idx])\n",
    "    \n",
    "    return ''.join(words)\n",
    "\n",
    "def generate_text(model, word_to_idx, idx_to_word, start_text, num_words_to_generate, max_sequence_length=30, device=\"cpu\", temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model with attention mechanism.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NextWordPredictor model\n",
    "        word_to_idx: Word to index mapping\n",
    "        idx_to_word: Index to word mapping\n",
    "        start_text: Initial text to start generation from\n",
    "        num_words_to_generate: Number of words to generate\n",
    "        max_sequence_length: Maximum sequence length for model input\n",
    "        device: Device to run inference on\n",
    "        temperature: Temperature for sampling (higher = more random)\n",
    "    \n",
    "    Returns:\n",
    "        generated_text: Complete generated text including start_text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated_ids = []\n",
    "    \n",
    "    # Encode the start text\n",
    "    current_input_ids = text_to_indices(start_text, word_to_idx, max_sequence_length)\n",
    "    \n",
    "    generated_ids.extend(current_input_ids)\n",
    "\n",
    "    # Convert to tensor and add batch dimension\n",
    "    input_tensor = torch.tensor([current_input_ids], dtype=torch.long).to(device)\n",
    "    hidden = model.init_hidden(1, device)\n",
    "    print(f\"Generating from: '{start_text}'\")\n",
    "\n",
    "    for _ in range(num_words_to_generate):\n",
    "        with torch.no_grad():\n",
    "            output_logits, hidden = model(input_tensor, hidden)\n",
    "            last_word_logits = output_logits[0, -1, :]  # (vocab_size,)\n",
    "            probabilities = torch.softmax(last_word_logits / temperature, dim=-1)\n",
    "\n",
    "            predicted_id = torch.multinomial(probabilities, 1).item()\n",
    "            generated_ids.append(predicted_id)\n",
    "\n",
    "            current_input_ids.append(predicted_id)\n",
    "\n",
    "            if len(current_input_ids) > max_sequence_length:\n",
    "                current_input_ids = current_input_ids[1:]\n",
    "\n",
    "            input_tensor = torch.tensor([current_input_ids], dtype=torch.long).to(device)\n",
    "\n",
    "    generated_text = indices_to_text(generated_ids, idx_to_word)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a54aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '1661-0.txt' already exists. Skipping download.\n",
      "Data files and tokenizer already exist. Skipping processing.\n",
      "Vocabulary size: 4436\n"
     ]
    }
   ],
   "source": [
    "download_sherlock_holmes_text()\n",
    "prepare_data_and_embeddings()\n",
    "with open(\"embeddings.pkl\", 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "with open(\"data.pkl\", 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "train_sequences = data['train_sequences']\n",
    "val_sequences = data['val_sequences']\n",
    "test_sequences = data['test_sequences']\n",
    "\n",
    "word_to_idx = embeddings['word_to_idx']\n",
    "idx_to_word = embeddings['idx_to_word']\n",
    "embeddings_matrix = embeddings['embeddings_matrix']\n",
    "\n",
    "pad_token_id = word_to_idx[\"<PAD>\"]\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "embedding_dim = embeddings_matrix.shape[1]\n",
    "batch_size = 64\n",
    "\n",
    "X_train, y_train = create_sequences_and_targets(train_sequences)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_val, y_val = create_sequences_and_targets(val_sequences)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X_test, y_test = create_sequences_and_targets(test_sequences)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0baf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextWordPredictor(\n",
       "  (embedding): Embedding(4436, 384, padding_idx=0)\n",
       "  (lstm): LSTM(384, 256, num_layers=2, batch_first=True)\n",
       "  (attention): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (fc): Linear(in_features=256, out_features=4436, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NextWordPredictor(vocab_size, embeddings_matrix, hidden_dim=256, num_layers=2, pad_token_id=pad_token_id)\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c7dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/10] Train Loss: 4.0168, Validation Loss: 3.4807\n",
      "Epoch [2/10] Train Loss: 3.1802, Validation Loss: 3.0256\n",
      "Epoch [3/10] Train Loss: 2.9106, Validation Loss: 2.8451\n",
      "Epoch [4/10] Train Loss: 2.7349, Validation Loss: 2.6991\n",
      "Calculating metrics...\n",
      "Epoch [5/10]:\n",
      "  Train Loss: 2.6042, Val Loss: 2.6127\n",
      "  Train Accuracy: 0.5424, Val Accuracy: 0.5368\n",
      "  Train Perplexity: 12.56, Val Perplexity: 13.63\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_accuracy, test_perplexity \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_dataloader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 169\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataloader, val_dataloader, num_epochs, vocab_size, device, clip_grad_norm)\u001b[0m\n\u001b[0;32m    167\u001b[0m hidden \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_hidden(batch_size, device)\n\u001b[0;32m    168\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 169\u001b[0m output_logits, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m hidden \u001b[38;5;241m=\u001b[39m (hidden[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach(), hidden[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m    172\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\achpi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\achpi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 98\u001b[0m, in \u001b[0;36mNextWordPredictor.forward\u001b[1;34m(self, input_ids, hidden)\u001b[0m\n\u001b[0;32m     95\u001b[0m context_vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(context_vectors, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, seq_len, hidden_dim)\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# 4. Fully connected layer\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_vector\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, seq_len, vocab_size)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, hidden\n",
      "File \u001b[1;32mc:\\Users\\achpi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\achpi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\achpi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, val_dataloader, num_epochs=10, vocab_size=vocab_size)\n",
    "\n",
    "test_accuracy, test_perplexity = evaluate_model(model, test_dataloader, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495cadc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx[\" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050249be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from: 'He never spoke of the softer passions'\n",
      "\n",
      "--- Generated Text ---\n",
      "He never spoke of the <UNK> <UNK> of     <UNK>  we<UNK> <UNK>   ,    . I   questioning,     , broad   <UNK>  he ,   \n"
     ]
    }
   ],
   "source": [
    "start_seed_text = \"He never spoke of the softer passions\"\n",
    "num_words = 50 \n",
    "generated_output = generate_text(model, word_to_idx, idx_to_word, start_seed_text, num_words, temperature=0.7)\n",
    "\n",
    "print(\"\\n--- Generated Text ---\")\n",
    "print(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4a7186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
