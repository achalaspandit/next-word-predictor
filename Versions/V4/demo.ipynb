{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import re\n",
        "import random"
      ],
      "metadata": {
        "id": "52YaS3rBs0X0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences_and_targets(encoded_sentences_ids, max_sequence_length=30):\n",
        "    X_samples = []\n",
        "    y_samples = []\n",
        "\n",
        "    for sentence_ids in encoded_sentences_ids:\n",
        "        if len(sentence_ids) < max_sequence_length + 1:\n",
        "            continue\n",
        "        # Sliding window within each sentence\n",
        "        for i in range(len(sentence_ids) - max_sequence_length):\n",
        "            input_seq = sentence_ids[i:i+max_sequence_length]\n",
        "            target_seq = sentence_ids[i+1:i+max_sequence_length+1]\n",
        "\n",
        "            X_samples.append(input_seq)\n",
        "            y_samples.append(target_seq)\n",
        "\n",
        "    X = torch.tensor(X_samples, dtype=torch.long)\n",
        "    y = torch.tensor(y_samples, dtype=torch.long)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "class NextWordPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network for next word prediction with the following architecture:\n",
        "    Embedding layer -> LSTM layer -> Attention layer -> Fully connected layer\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embeddings_matrix, hidden_dim, num_layers, pad_token_id):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embeddings_matrix.shape[1]\n",
        "        self.embeddings_matrix = torch.from_numpy(embeddings_matrix).float()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(self.embeddings_matrix, freeze=False, padding_idx=self.pad_token_id)\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
        "        self.attention = nn.Linear(self.hidden_dim, 1)\n",
        "        self.fc = nn.Linear(self.hidden_dim * 2, self.vocab_size)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids, hidden=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        embedded = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)  # (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        attended_output = torch.bmm(attention_weights.transpose(1, 2), lstm_out)\n",
        "        combined = torch.cat([lstm_out, attended_output.expand_as(lstm_out)], dim=-1)\n",
        "\n",
        "        output = self.fc(combined) # (batch_size, hidden_dim*2, vocab_size)\n",
        "        # output = self.dropout(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
        "\n",
        "def calculate_accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            hidden = model.init_hidden(batch_size, device)\n",
        "            output_logits, hidden = model(inputs, hidden)\n",
        "\n",
        "            predictions = torch.argmax(output_logits, dim=-1)  # (batch_size, seq_len)\n",
        "\n",
        "            correct_predictions += (predictions == targets).sum().item()\n",
        "            total_predictions += targets.numel()\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "def calculate_perplexity(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token_id, reduction='sum')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            hidden = model.init_hidden(batch_size, device)\n",
        "            output_logits, hidden = model(inputs, hidden)\n",
        "\n",
        "            loss = criterion(output_logits.view(-1, model.vocab_size), targets.view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += targets.numel()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, num_epochs, vocab_size, device=\"cpu\", clip_grad_norm=1.0):\n",
        "    model.train()\n",
        "    print(\"Starting training...\")\n",
        "    metrics_history = []\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.00001)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dataloader), epochs=num_epochs, pct_start=0.1)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token_id)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "            hidden = model.init_hidden(batch_size, device)\n",
        "            optimizer.zero_grad()\n",
        "            output_logits, hidden = model(inputs, hidden)\n",
        "\n",
        "            hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "            loss = criterion(output_logits.view(-1, vocab_size), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for inputs, targets in val_dataloader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                batch_size = inputs.size(0)\n",
        "                val_hidden = model.init_hidden(batch_size, device)\n",
        "                output_logits, val_hidden = model(inputs, val_hidden)\n",
        "                val_hidden = (val_hidden[0].detach(), val_hidden[1].detach())\n",
        "\n",
        "                loss = criterion(output_logits.view(-1, vocab_size), targets.view(-1))\n",
        "                val_loss += loss.item()\n",
        "        avg_val_loss = val_loss / len(val_dataloader)\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate accuracy and perplexity every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(\"Calculating metrics...\")\n",
        "            train_accuracy = calculate_accuracy(model, train_dataloader, device)\n",
        "            val_accuracy = calculate_accuracy(model, val_dataloader, device)\n",
        "\n",
        "            train_perplexity = calculate_perplexity(model, train_dataloader, device)\n",
        "            val_perplexity = calculate_perplexity(model, val_dataloader, device)\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n",
        "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "            print(f\"  Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "            print(f\"  Train Perplexity: {train_perplexity:.2f}, Val Perplexity: {val_perplexity:.2f}\")\n",
        "            print(\"-\" * 60)\n",
        "            metrics_history.append({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": avg_train_loss,\n",
        "                \"val_loss\": avg_val_loss,\n",
        "                \"train_accuracy\": train_accuracy,\n",
        "                \"val_accuracy\": val_accuracy,\n",
        "                \"train_perplexity\": train_perplexity,\n",
        "                \"val_perplexity\": val_perplexity\n",
        "            })\n",
        "        else:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    return metrics_history\n",
        "\n",
        "def evaluate_model(model, test_dataloader, device):\n",
        "    print(\"Evaluating model on test set...\")\n",
        "\n",
        "    test_accuracy = calculate_accuracy(model, test_dataloader, device)\n",
        "    test_perplexity = calculate_perplexity(model, test_dataloader, device)\n",
        "\n",
        "    print(f\"Final Test Results:\")\n",
        "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"  Test Perplexity: {test_perplexity:.2f}\")\n",
        "\n",
        "    return test_accuracy, test_perplexity\n",
        "\n",
        "def text_to_indices(text, word_to_idx, max_sequence_length=30):\n",
        "    words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text.lower())\n",
        "    indices = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in word_to_idx:\n",
        "            indices.append(word_to_idx[word])\n",
        "        else:\n",
        "            indices.append(word_to_idx['<UNK>'])\n",
        "\n",
        "    if max_sequence_length and len(indices) > max_sequence_length:\n",
        "        indices = indices[-max_sequence_length:]\n",
        "\n",
        "    return indices\n",
        "\n",
        "def indices_to_text(indices, idx_to_word):\n",
        "    words = []\n",
        "    special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']\n",
        "    for idx in indices:\n",
        "        if idx in idx_to_word:\n",
        "            if idx_to_word[idx] in special_tokens:\n",
        "                continue\n",
        "            words.append(idx_to_word[idx])\n",
        "    content = ''.join(words)\n",
        "    content = re.sub(r'\\s+', ' ', content).strip()\n",
        "    return content\n",
        "\n",
        "def generate_text(model, word_to_idx, idx_to_word, start_text, num_words_to_generate, top5= False, max_sequence_length=30, device=\"cpu\", temperature=1.0):\n",
        "\n",
        "    model.eval()\n",
        "    generated_ids = []\n",
        "    prediction_details = []\n",
        "\n",
        "    # Encode the start text\n",
        "    current_input_ids = text_to_indices(start_text, word_to_idx, max_sequence_length)\n",
        "    generated_ids.extend(current_input_ids)\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    input_tensor = torch.tensor([current_input_ids], dtype=torch.long).to(device)\n",
        "    hidden = model.init_hidden(1, device)\n",
        "    print(f\"Generating from: '{start_text}'\")\n",
        "\n",
        "    for i in range(num_words_to_generate):\n",
        "        with torch.no_grad():\n",
        "            output_logits, hidden = model(input_tensor, hidden)\n",
        "            last_word_logits = output_logits[0, -1, :]\n",
        "            probabilities = torch.softmax(last_word_logits / temperature, dim=-1)\n",
        "\n",
        "            top_5_probs, top_5_indices = torch.topk(probabilities, 5)\n",
        "            top_5_words_and_probs = []\n",
        "            for j in range(len(top_5_indices)):\n",
        "                word = idx_to_word.get(top_5_indices[j].item(), '<UNK>')\n",
        "                prob = top_5_probs[j].item()\n",
        "                top_5_words_and_probs.append(f\"{word} ({prob:.4f})\")\n",
        "\n",
        "            top_k = 50\n",
        "            top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "            top_k_probs = top_k_probs / top_k_probs.sum()  # re-normalize\n",
        "\n",
        "            predicted_id = torch.multinomial(top_k_probs, 1).item()\n",
        "            predicted_id = top_k_indices[predicted_id].item()\n",
        "\n",
        "            prediction_details.append({\n",
        "                \"step\": i + 1,\n",
        "                \"top_5_alternatives\": top_5_words_and_probs\n",
        "            })\n",
        "            cnt = 0\n",
        "            while cnt<3:\n",
        "                if predicted_id == word_to_idx[' ']:\n",
        "                    break\n",
        "                if predicted_id in generated_ids:\n",
        "                    x = random.randint(0, 4)\n",
        "                    predicted_id = top_5_indices[x].item()\n",
        "                    cnt+=1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            generated_ids.append(predicted_id)\n",
        "\n",
        "            current_input_ids.append(predicted_id)\n",
        "\n",
        "            if len(current_input_ids) > max_sequence_length:\n",
        "                current_input_ids = current_input_ids[1:]\n",
        "\n",
        "            input_tensor = torch.tensor([current_input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    generated_text = indices_to_text(generated_ids, idx_to_word)\n",
        "    if top5 == True:\n",
        "      print(\"\\n--- Prediction Details ---\")\n",
        "      for detail in prediction_details:\n",
        "          print(f\"Step {detail['step']}:'\")\n",
        "          print(f\"  Top 5 likely words: {', '.join(detail['top_5_alternatives'])}\")\n",
        "      print(\"--------------------------\\n\")\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "5Nvo9n4ztCYX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load data and embeddings from pickle file\n",
        "with open(\"embeddings.pkl\", 'rb') as f:\n",
        "    embeddings = pickle.load(f)\n",
        "with open(\"data.pkl\", 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "train_sequences = data['train_sequences']\n",
        "val_sequences = data['val_sequences']\n",
        "test_sequences = data['test_sequences']\n",
        "\n",
        "word_to_idx = embeddings['word_to_idx']\n",
        "idx_to_word = embeddings['idx_to_word']\n",
        "embeddings_matrix = embeddings['embeddings_matrix']\n",
        "\n",
        "# 2. Initialize parameters\n",
        "pad_token_id = word_to_idx[\"<PAD>\"]\n",
        "vocab_size = len(word_to_idx)\n",
        "embedding_dim = embeddings_matrix.shape[1]\n",
        "batch_size = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 3. Prepare train, val, test data loaders\n",
        "X_train, y_train = create_sequences_and_targets(train_sequences)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "X_val, y_val = create_sequences_and_targets(val_sequences)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "X_test, y_test = create_sequences_and_targets(test_sequences)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#4. Initialize, train and evaluate model\n",
        "model = NextWordPredictor(vocab_size, embeddings_matrix, hidden_dim=256, num_layers=1, pad_token_id=pad_token_id)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "hDcVJI-jj7CH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b0bdee-f4ed-4d54-f079-fa710aaae6fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NextWordPredictor(\n",
              "  (embedding): Embedding(4389, 384, padding_idx=0)\n",
              "  (lstm): LSTM(384, 256, batch_first=True)\n",
              "  (attention): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (fc): Linear(in_features=512, out_features=4389, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = train_model(model, train_dataloader, val_dataloader, num_epochs=100, vocab_size=vocab_size, device=device)\n",
        "evaluate_model(model, test_dataloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsiEfiU2uuS4",
        "outputId": "7ad87251-e6cb-47ff-df18-0cc0c12a9ebb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch [1/100] Train Loss: 3.9821, Validation Loss: 3.1946\n",
            "Epoch [2/100] Train Loss: 3.0914, Validation Loss: 3.0590\n",
            "Epoch [3/100] Train Loss: 2.9588, Validation Loss: 2.9389\n",
            "Epoch [4/100] Train Loss: 2.8371, Validation Loss: 2.8246\n",
            "Epoch [5/100] Train Loss: 2.7071, Validation Loss: 2.7099\n",
            "Epoch [6/100] Train Loss: 2.5892, Validation Loss: 2.6167\n",
            "Epoch [7/100] Train Loss: 2.4894, Validation Loss: 2.5394\n",
            "Epoch [8/100] Train Loss: 2.4021, Validation Loss: 2.4745\n",
            "Epoch [9/100] Train Loss: 2.3258, Validation Loss: 2.4196\n",
            "Calculating metrics...\n",
            "Epoch [10/100]:\n",
            "  Train Loss: 2.2574, Val Loss: 2.3702\n",
            "  Train Accuracy: 0.5983, Val Accuracy: 0.5885\n",
            "  Train Perplexity: 9.16, Val Perplexity: 10.69\n",
            "------------------------------------------------------------\n",
            "Epoch [11/100] Train Loss: 2.1934, Validation Loss: 2.3252\n",
            "Epoch [12/100] Train Loss: 2.1325, Validation Loss: 2.2819\n",
            "Epoch [13/100] Train Loss: 2.0733, Validation Loss: 2.2401\n",
            "Epoch [14/100] Train Loss: 2.0161, Validation Loss: 2.1999\n",
            "Epoch [15/100] Train Loss: 1.9609, Validation Loss: 2.1626\n",
            "Epoch [16/100] Train Loss: 1.9077, Validation Loss: 2.1268\n",
            "Epoch [17/100] Train Loss: 1.8562, Validation Loss: 2.0923\n",
            "Epoch [18/100] Train Loss: 1.8069, Validation Loss: 2.0611\n",
            "Epoch [19/100] Train Loss: 1.7597, Validation Loss: 2.0303\n",
            "Calculating metrics...\n",
            "Epoch [20/100]:\n",
            "  Train Loss: 1.7140, Val Loss: 2.0024\n",
            "  Train Accuracy: 0.6724, Val Accuracy: 0.6461\n",
            "  Train Perplexity: 5.31, Val Perplexity: 7.40\n",
            "------------------------------------------------------------\n",
            "Epoch [21/100] Train Loss: 1.6704, Validation Loss: 1.9758\n",
            "Epoch [22/100] Train Loss: 1.6283, Validation Loss: 1.9495\n",
            "Epoch [23/100] Train Loss: 1.5873, Validation Loss: 1.9248\n",
            "Epoch [24/100] Train Loss: 1.5478, Validation Loss: 1.9013\n",
            "Epoch [25/100] Train Loss: 1.5093, Validation Loss: 1.8778\n",
            "Epoch [26/100] Train Loss: 1.4720, Validation Loss: 1.8563\n",
            "Epoch [27/100] Train Loss: 1.4357, Validation Loss: 1.8346\n",
            "Epoch [28/100] Train Loss: 1.4003, Validation Loss: 1.8133\n",
            "Epoch [29/100] Train Loss: 1.3660, Validation Loss: 1.7933\n",
            "Calculating metrics...\n",
            "Epoch [30/100]:\n",
            "  Train Loss: 1.3327, Val Loss: 1.7728\n",
            "  Train Accuracy: 0.7353, Val Accuracy: 0.6818\n",
            "  Train Perplexity: 3.61, Val Perplexity: 5.88\n",
            "------------------------------------------------------------\n",
            "Epoch [31/100] Train Loss: 1.3002, Validation Loss: 1.7535\n",
            "Epoch [32/100] Train Loss: 1.2688, Validation Loss: 1.7342\n",
            "Epoch [33/100] Train Loss: 1.2383, Validation Loss: 1.7155\n",
            "Epoch [34/100] Train Loss: 1.2082, Validation Loss: 1.6976\n",
            "Epoch [35/100] Train Loss: 1.1790, Validation Loss: 1.6794\n",
            "Epoch [36/100] Train Loss: 1.1507, Validation Loss: 1.6623\n",
            "Epoch [37/100] Train Loss: 1.1230, Validation Loss: 1.6458\n",
            "Epoch [38/100] Train Loss: 1.0962, Validation Loss: 1.6291\n",
            "Epoch [39/100] Train Loss: 1.0700, Validation Loss: 1.6131\n",
            "Calculating metrics...\n",
            "Epoch [40/100]:\n",
            "  Train Loss: 1.0446, Val Loss: 1.5973\n",
            "  Train Accuracy: 0.7899, Val Accuracy: 0.7105\n",
            "  Train Perplexity: 2.69, Val Perplexity: 4.93\n",
            "------------------------------------------------------------\n",
            "Epoch [41/100] Train Loss: 1.0198, Validation Loss: 1.5815\n",
            "Epoch [42/100] Train Loss: 0.9956, Validation Loss: 1.5666\n",
            "Epoch [43/100] Train Loss: 0.9722, Validation Loss: 1.5520\n",
            "Epoch [44/100] Train Loss: 0.9498, Validation Loss: 1.5374\n",
            "Epoch [45/100] Train Loss: 0.9274, Validation Loss: 1.5231\n",
            "Epoch [46/100] Train Loss: 0.9060, Validation Loss: 1.5098\n",
            "Epoch [47/100] Train Loss: 0.8852, Validation Loss: 1.4968\n",
            "Epoch [48/100] Train Loss: 0.8649, Validation Loss: 1.4831\n",
            "Epoch [49/100] Train Loss: 0.8451, Validation Loss: 1.4709\n",
            "Calculating metrics...\n",
            "Epoch [50/100]:\n",
            "  Train Loss: 0.8259, Val Loss: 1.4576\n",
            "  Train Accuracy: 0.8348, Val Accuracy: 0.7350\n",
            "  Train Perplexity: 2.15, Val Perplexity: 4.29\n",
            "------------------------------------------------------------\n",
            "Epoch [51/100] Train Loss: 0.8074, Validation Loss: 1.4460\n",
            "Epoch [52/100] Train Loss: 0.7894, Validation Loss: 1.4341\n",
            "Epoch [53/100] Train Loss: 0.7717, Validation Loss: 1.4218\n",
            "Epoch [54/100] Train Loss: 0.7546, Validation Loss: 1.4105\n",
            "Epoch [55/100] Train Loss: 0.7383, Validation Loss: 1.3997\n",
            "Epoch [56/100] Train Loss: 0.7221, Validation Loss: 1.3886\n",
            "Epoch [57/100] Train Loss: 0.7066, Validation Loss: 1.3784\n",
            "Epoch [58/100] Train Loss: 0.6913, Validation Loss: 1.3682\n",
            "Epoch [59/100] Train Loss: 0.6766, Validation Loss: 1.3584\n",
            "Calculating metrics...\n",
            "Epoch [60/100]:\n",
            "  Train Loss: 0.6624, Val Loss: 1.3485\n",
            "  Train Accuracy: 0.8700, Val Accuracy: 0.7558\n",
            "  Train Perplexity: 1.82, Val Perplexity: 3.85\n",
            "------------------------------------------------------------\n",
            "Epoch [61/100] Train Loss: 0.6486, Validation Loss: 1.3382\n",
            "Epoch [62/100] Train Loss: 0.6351, Validation Loss: 1.3289\n",
            "Epoch [63/100] Train Loss: 0.6216, Validation Loss: 1.3195\n",
            "Epoch [64/100] Train Loss: 0.6089, Validation Loss: 1.3127\n",
            "Epoch [65/100] Train Loss: 0.5965, Validation Loss: 1.3034\n",
            "Epoch [66/100] Train Loss: 0.5843, Validation Loss: 1.2951\n",
            "Epoch [67/100] Train Loss: 0.5727, Validation Loss: 1.2868\n",
            "Epoch [68/100] Train Loss: 0.5613, Validation Loss: 1.2790\n",
            "Epoch [69/100] Train Loss: 0.5501, Validation Loss: 1.2715\n",
            "Calculating metrics...\n",
            "Epoch [70/100]:\n",
            "  Train Loss: 0.5391, Val Loss: 1.2633\n",
            "  Train Accuracy: 0.8973, Val Accuracy: 0.7728\n",
            "  Train Perplexity: 1.61, Val Perplexity: 3.53\n",
            "------------------------------------------------------------\n",
            "Epoch [71/100] Train Loss: 0.5289, Validation Loss: 1.2567\n",
            "Epoch [72/100] Train Loss: 0.5183, Validation Loss: 1.2489\n",
            "Epoch [73/100] Train Loss: 0.5083, Validation Loss: 1.2423\n",
            "Epoch [74/100] Train Loss: 0.4987, Validation Loss: 1.2345\n",
            "Epoch [75/100] Train Loss: 0.4892, Validation Loss: 1.2280\n",
            "Epoch [76/100] Train Loss: 0.4800, Validation Loss: 1.2217\n",
            "Epoch [77/100] Train Loss: 0.4711, Validation Loss: 1.2155\n",
            "Epoch [78/100] Train Loss: 0.4623, Validation Loss: 1.2089\n",
            "Epoch [79/100] Train Loss: 0.4537, Validation Loss: 1.2032\n",
            "Calculating metrics...\n",
            "Epoch [80/100]:\n",
            "  Train Loss: 0.4455, Val Loss: 1.1977\n",
            "  Train Accuracy: 0.9186, Val Accuracy: 0.7874\n",
            "  Train Perplexity: 1.47, Val Perplexity: 3.31\n",
            "------------------------------------------------------------\n",
            "Epoch [81/100] Train Loss: 0.4373, Validation Loss: 1.1920\n",
            "Epoch [82/100] Train Loss: 0.4296, Validation Loss: 1.1862\n",
            "Epoch [83/100] Train Loss: 0.4218, Validation Loss: 1.1796\n",
            "Epoch [84/100] Train Loss: 0.4145, Validation Loss: 1.1752\n",
            "Epoch [85/100] Train Loss: 0.4073, Validation Loss: 1.1691\n",
            "Epoch [86/100] Train Loss: 0.4001, Validation Loss: 1.1642\n",
            "Epoch [87/100] Train Loss: 0.3935, Validation Loss: 1.1588\n",
            "Epoch [88/100] Train Loss: 0.3866, Validation Loss: 1.1554\n",
            "Epoch [89/100] Train Loss: 0.3799, Validation Loss: 1.1504\n",
            "Calculating metrics...\n",
            "Epoch [90/100]:\n",
            "  Train Loss: 0.3737, Val Loss: 1.1459\n",
            "  Train Accuracy: 0.9350, Val Accuracy: 0.7992\n",
            "  Train Perplexity: 1.37, Val Perplexity: 3.14\n",
            "------------------------------------------------------------\n",
            "Epoch [91/100] Train Loss: 0.3674, Validation Loss: 1.1412\n",
            "Epoch [92/100] Train Loss: 0.3612, Validation Loss: 1.1367\n",
            "Epoch [93/100] Train Loss: 0.3555, Validation Loss: 1.1325\n",
            "Epoch [94/100] Train Loss: 0.3494, Validation Loss: 1.1280\n",
            "Epoch [95/100] Train Loss: 0.3438, Validation Loss: 1.1250\n",
            "Epoch [96/100] Train Loss: 0.3384, Validation Loss: 1.1200\n",
            "Epoch [97/100] Train Loss: 0.3327, Validation Loss: 1.1177\n",
            "Epoch [98/100] Train Loss: 0.3278, Validation Loss: 1.1127\n",
            "Epoch [99/100] Train Loss: 0.3225, Validation Loss: 1.1088\n",
            "Calculating metrics...\n",
            "Epoch [100/100]:\n",
            "  Train Loss: 0.3174, Val Loss: 1.1047\n",
            "  Train Accuracy: 0.9474, Val Accuracy: 0.8085\n",
            "  Train Perplexity: 1.29, Val Perplexity: 3.01\n",
            "------------------------------------------------------------\n",
            "Training finished.\n",
            "Evaluating model on test set...\n",
            "Final Test Results:\n",
            "  Test Accuracy: 0.8040\n",
            "  Test Perplexity: 3.11\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8039602414248124, 3.1145761302854464)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_seed_text = [\"Boots which extended halfway\", \"I ordered him to pay\", \"I answered that it had\", \"remove crusted mud from it. Hence\", \"He never spoke of the\"]\n",
        "num_words = [50, 50, 50, 50, 10]\n",
        "top_5 = [False, False, False, False, True]\n",
        "for i in range(len(start_seed_text)):\n",
        "    print(f\"\\n--- Generating text for seed: '{start_seed_text[i]}' ---\")\n",
        "    generated_output = generate_text(model, word_to_idx, idx_to_word, start_seed_text[i], num_words[i], top5=top_5[i], temperature= 0.5, max_sequence_length=30, device=device)\n",
        "    print(f\"Generated text: {generated_output}\")\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "id": "fZvZkBbKCn-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XW0G0r-3CsY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}