NextWordPredictor(
  (embedding): Embedding(8535, 384, padding_idx=0)
  (lstm): LSTM(384, 256, num_layers=2, batch_first=True)
  (attention): LuongAttention(
    (W_a): Linear(in_features=256, out_features=256, bias=False)
  )
  (fc): Linear(in_features=256, out_features=8535, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
---------------------------------------------------
max_sequence_length = 50
-------------------------
Starting training...
Model architecture: Embedding -> LSTM -> Luong Attention -> FC
Hidden dim: 256, LSTM layers: 2
Epoch [1/10] Train Loss: 4.7349, Validation Loss: 4.1714
Epoch [2/10] Train Loss: 4.1758, Validation Loss: 4.1617
Epoch [3/10] Train Loss: 4.1506, Validation Loss: 4.1552
Epoch [4/10] Train Loss: 3.9288, Validation Loss: 3.5316
Calculating metrics...
Epoch [5/10]:
  Train Loss: 3.4508, Val Loss: 3.4581
  Train Accuracy: 0.4759, Val Accuracy: 0.4768
  Train Perplexity: 28.84, Val Perplexity: 31.72
------------------------------------------------------------
Epoch [6/10] Train Loss: 3.3945, Validation Loss: 3.4083
Epoch [7/10] Train Loss: 3.3177, Validation Loss: 3.3343
Epoch [8/10] Train Loss: 3.2275, Validation Loss: 3.2361
Epoch [9/10] Train Loss: 3.1391, Validation Loss: 3.1870
Calculating metrics...
Epoch [10/10]:
  Train Loss: 3.0783, Val Loss: 3.1478
  Train Accuracy: 0.5012, Val Accuracy: 0.5001
  Train Perplexity: 20.15, Val Perplexity: 23.23
------------------------------------------------------------
Training finished.
Evaluating model on test set...
Final Test Results:
  Test Accuracy: 0.4992
  Test Perplexity: 22.58

Assignment Requirements Check:
  Test Accuracy > 75%: ✗ (49.9%)
  Perplexity < 250: ✓ (22.6)
Generating from: 'which must always be associated'

--- Generated Text ---
which must always be associated as a motive to year to men the happy if it I a trace in you last was of to to the have the told

---------------------------------------------------
max_sequence_length = 100
-------------------------
Starting training...
Model architecture: Embedding -> LSTM -> Luong Attention -> FC
Hidden dim: 256, LSTM layers: 2
Epoch [1/10] Train Loss: 5.8047, Validation Loss: 4.2467
Epoch [2/10] Train Loss: 4.2396, Validation Loss: 4.2162
Epoch [3/10] Train Loss: 4.1798, Validation Loss: 4.1952
Epoch [4/10] Train Loss: 4.1521, Validation Loss: 4.1926
Calculating metrics...
Epoch [5/10]:
  Train Loss: 4.1341, Val Loss: 4.1918
  Train Accuracy: 0.4534, Val Accuracy: 0.4546
  Train Perplexity: 57.79, Val Perplexity: 63.04
------------------------------------------------------------
Epoch [6/10] Train Loss: 4.1233, Validation Loss: 4.1947
Epoch [7/10] Train Loss: 4.1149, Validation Loss: 4.1974
Epoch [8/10] Train Loss: 4.1113, Validation Loss: 4.1959
Epoch [9/10] Train Loss: 4.1060, Validation Loss: 4.2096
Calculating metrics...
Epoch [10/10]:
  Train Loss: 4.0947, Val Loss: 4.2037
  Train Accuracy: 0.4534, Val Accuracy: 0.4546
  Train Perplexity: 57.51, Val Perplexity: 63.79
------------------------------------------------------------
Training finished.
Evaluating model on test set...
...
Generating from: 'which must always be associated'

--- Generated Text ---
which must always be associated  , have have.    hisI said- You    be   “         with           but


---------------------------------------------------
max_sequence_length = 150
-------------------------
Starting training...
Model architecture: Embedding -> LSTM -> Luong Attention -> FC
Hidden dim: 256, LSTM layers: 2
Epoch [1/10] Train Loss: 8.6637, Validation Loss: 6.5621
Epoch [2/10] Train Loss: 5.1594, Validation Loss: 4.4008
Epoch [3/10] Train Loss: 4.3587, Validation Loss: 4.2209
Epoch [4/10] Train Loss: 4.2091, Validation Loss: 4.1732
Calculating metrics...
Epoch [5/10]:
  Train Loss: 4.1676, Val Loss: 4.1747
  Train Accuracy: 0.4540, Val Accuracy: 0.4553
  Train Perplexity: 57.80, Val Perplexity: 65.12
------------------------------------------------------------
Epoch [6/10] Train Loss: 4.1381, Validation Loss: 4.1865
Epoch [7/10] Train Loss: 4.1237, Validation Loss: 4.1710
Epoch [8/10] Train Loss: 4.1162, Validation Loss: 4.1764
Epoch [9/10] Train Loss: 4.1053, Validation Loss: 4.1735
Calculating metrics...
Epoch [10/10]:
  Train Loss: 4.1001, Val Loss: 4.1747
  Train Accuracy: 0.4540, Val Accuracy: 0.4553
  Train Perplexity: 56.57, Val Perplexity: 65.40
------------------------------------------------------------
Training finished.
Evaluating model on test set...
Final Test Results:
  Test Accuracy: 0.4545
  Test Perplexity: 64.11

Assignment Requirements Check:
  Test Accuracy > 75%: ✗ (45.4%)
  Perplexity < 250: ✓ (64.1)
Generating from: 'which must always be associated'

--- Generated Text ---
which must always be associated      Ithatgive         to  ofis,        was               deepa


--------------------------------------------------------------------------------------
NextWordPredictor(
  (embedding): Embedding(8535, 384, padding_idx=0)
  (lstm): LSTM(384, 512, num_layers=2, batch_first=True)
  (attention): LuongAttention(
    (W_a): Linear(in_features=512, out_features=512, bias=False)
  )
  (fc): Linear(in_features=512, out_features=8535, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)

---------------------------------------------------
max_sequence_length = 75
-------------------------
Starting training...
Model architecture: Embedding -> LSTM -> Luong Attention -> FC
Hidden dim: 512, LSTM layers: 2
Epoch [1/10] Train Loss: 4.7741, Validation Loss: 4.1630
Epoch [2/10] Train Loss: 4.1515, Validation Loss: 4.1351
Epoch [3/10] Train Loss: 4.1235, Validation Loss: 4.1327
Epoch [4/10] Train Loss: 4.1086, Validation Loss: 4.1308
Calculating metrics...
Epoch [5/10]:
  Train Loss: 3.5631, Val Loss: 3.4746
  Train Accuracy: 0.4757, Val Accuracy: 0.4776
  Train Perplexity: 29.96, Val Perplexity: 32.32
------------------------------------------------------------
Epoch [6/10] Train Loss: 3.4245, Validation Loss: 3.4332
Epoch [7/10] Train Loss: 3.3657, Validation Loss: 3.3712
Epoch [8/10] Train Loss: 3.2953, Validation Loss: 3.3330
Epoch [9/10] Train Loss: 3.2461, Validation Loss: 3.2705
Calculating metrics...
Epoch [10/10]:
  Train Loss: 3.1639, Val Loss: 3.1936
  Train Accuracy: 0.4955, Val Accuracy: 0.4966
  Train Perplexity: 21.93, Val Perplexity: 24.42
------------------------------------------------------------
Training finished.
Evaluating model on test set...
...
Generating from: 'which must always be associated'

--- Generated Text ---
which must always be associated are the have the saw of, and knew to me in that the., be will which old a Sutherland of to the’ “
