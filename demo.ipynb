{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import re\n",
        "import random"
      ],
      "metadata": {
        "id": "52YaS3rBs0X0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences_and_targets(encoded_sentences_ids, max_sequence_length=30):\n",
        "    X_samples = []\n",
        "    y_samples = []\n",
        "\n",
        "    for sentence_ids in encoded_sentences_ids:\n",
        "        if len(sentence_ids) < max_sequence_length + 1:\n",
        "            continue\n",
        "        # Sliding window within each sentence\n",
        "        for i in range(len(sentence_ids) - max_sequence_length):\n",
        "            input_seq = sentence_ids[i:i+max_sequence_length]\n",
        "            target_seq = sentence_ids[i+1:i+max_sequence_length+1]\n",
        "\n",
        "            X_samples.append(input_seq)\n",
        "            y_samples.append(target_seq)\n",
        "\n",
        "    X = torch.tensor(X_samples, dtype=torch.long)\n",
        "    y = torch.tensor(y_samples, dtype=torch.long)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "class NextWordPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network for next word prediction with the following architecture:\n",
        "    Embedding layer -> LSTM layer -> Attention layer -> Fully connected layer\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embeddings_matrix, hidden_dim, num_layers, pad_token_id):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embeddings_matrix.shape[1]\n",
        "        self.embeddings_matrix = torch.from_numpy(embeddings_matrix).float()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(self.embeddings_matrix, freeze=False, padding_idx=self.pad_token_id)\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
        "        self.attention = nn.Linear(self.hidden_dim, 1)\n",
        "        self.fc = nn.Linear(self.hidden_dim * 2, self.vocab_size)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids, hidden=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        embedded = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)  # (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        attended_output = torch.bmm(attention_weights.transpose(1, 2), lstm_out)\n",
        "        combined = torch.cat([lstm_out, attended_output.expand_as(lstm_out)], dim=-1)\n",
        "\n",
        "        output = self.fc(combined) # (batch_size, hidden_dim*2, vocab_size)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
        "\n",
        "def calculate_accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            hidden = model.init_hidden(batch_size, device)\n",
        "            output_logits, hidden = model(inputs, hidden)\n",
        "\n",
        "            predictions = torch.argmax(output_logits, dim=-1)  # (batch_size, seq_len)\n",
        "\n",
        "            correct_predictions += (predictions == targets).sum().item()\n",
        "            total_predictions += targets.numel()\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "def calculate_perplexity(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token_id, reduction='sum')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "\n",
        "            hidden = model.init_hidden(batch_size, device)\n",
        "            output_logits, hidden = model(inputs, hidden)\n",
        "\n",
        "            loss = criterion(output_logits.view(-1, model.vocab_size), targets.view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += targets.numel()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, num_epochs, vocab_size, device=\"cpu\", clip_grad_norm=1.0):\n",
        "    model.train()\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.00001)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dataloader), epochs=num_epochs, pct_start=0.1)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_token_id)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size = inputs.size(0)\n",
        "            hidden = model.init_hidden(batch_size, device)\n",
        "            optimizer.zero_grad()\n",
        "            output_logits, hidden = model(inputs, hidden)\n",
        "\n",
        "            hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "            loss = criterion(output_logits.view(-1, vocab_size), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for inputs, targets in val_dataloader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                batch_size = inputs.size(0)\n",
        "                val_hidden = model.init_hidden(batch_size, device)\n",
        "                output_logits, val_hidden = model(inputs, val_hidden)\n",
        "                val_hidden = (val_hidden[0].detach(), val_hidden[1].detach())\n",
        "\n",
        "                loss = criterion(output_logits.view(-1, vocab_size), targets.view(-1))\n",
        "                val_loss += loss.item()\n",
        "        avg_val_loss = val_loss / len(val_dataloader)\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate accuracy and perplexity every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(\"Calculating metrics...\")\n",
        "            train_accuracy = calculate_accuracy(model, train_dataloader, device)\n",
        "            val_accuracy = calculate_accuracy(model, val_dataloader, device)\n",
        "\n",
        "            train_perplexity = calculate_perplexity(model, train_dataloader, device)\n",
        "            val_perplexity = calculate_perplexity(model, val_dataloader, device)\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n",
        "            print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "            print(f\"  Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "            print(f\"  Train Perplexity: {train_perplexity:.2f}, Val Perplexity: {val_perplexity:.2f}\")\n",
        "            print(\"-\" * 60)\n",
        "        else:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "def evaluate_model(model, test_dataloader, device):\n",
        "    print(\"Evaluating model on test set...\")\n",
        "\n",
        "    test_accuracy = calculate_accuracy(model, test_dataloader, device)\n",
        "    test_perplexity = calculate_perplexity(model, test_dataloader, device)\n",
        "\n",
        "    print(f\"Final Test Results:\")\n",
        "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"  Test Perplexity: {test_perplexity:.2f}\")\n",
        "\n",
        "    return test_accuracy, test_perplexity\n",
        "\n",
        "def text_to_indices(text, word_to_idx, max_sequence_length=30):\n",
        "    words = re.findall(r'\\w+|[^\\w\\s]|\\s+', text.lower())\n",
        "    indices = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in word_to_idx:\n",
        "            indices.append(word_to_idx[word])\n",
        "        else:\n",
        "            indices.append(word_to_idx['<UNK>'])\n",
        "\n",
        "    if max_sequence_length and len(indices) > max_sequence_length:\n",
        "        indices = indices[-max_sequence_length:]\n",
        "\n",
        "    return indices\n",
        "\n",
        "def indices_to_text(indices, idx_to_word):\n",
        "    words = []\n",
        "    special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']\n",
        "    for idx in indices:\n",
        "        if idx in idx_to_word:\n",
        "            if idx_to_word[idx] in special_tokens:\n",
        "                continue\n",
        "            words.append(idx_to_word[idx])\n",
        "    content = ''.join(words)\n",
        "    content = re.sub(r'\\s+', ' ', content).strip()\n",
        "    return content\n",
        "\n",
        "def generate_text(model, word_to_idx, idx_to_word, start_text, num_words_to_generate, top5= False, max_sequence_length=30, device=\"cpu\", temperature=1.0):\n",
        "\n",
        "    model.eval()\n",
        "    generated_ids = []\n",
        "    prediction_details = []\n",
        "\n",
        "    # Encode the start text\n",
        "    current_input_ids = text_to_indices(start_text, word_to_idx, max_sequence_length)\n",
        "    generated_ids.extend(current_input_ids)\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    input_tensor = torch.tensor([current_input_ids], dtype=torch.long).to(device)\n",
        "    hidden = model.init_hidden(1, device)\n",
        "    print(f\"Generating from: '{start_text}'\")\n",
        "\n",
        "    for i in range(num_words_to_generate):\n",
        "        with torch.no_grad():\n",
        "            output_logits, hidden = model(input_tensor, hidden)\n",
        "            last_word_logits = output_logits[0, -1, :]\n",
        "            probabilities = torch.softmax(last_word_logits / temperature, dim=-1)\n",
        "\n",
        "            top_5_probs, top_5_indices = torch.topk(probabilities, 5)\n",
        "            top_5_words_and_probs = []\n",
        "            top5d = dict()\n",
        "            for j in range(len(top_5_indices)):\n",
        "                word = idx_to_word.get(top_5_indices[j].item(), '<UNK>')\n",
        "                prob = top_5_probs[j].item()\n",
        "                top_5_words_and_probs.append(f\"{word} ({prob:.4f})\")\n",
        "                top5d[word] = prob\n",
        "\n",
        "            predicted_id = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            prediction_details.append({\n",
        "                \"step\": i + 1,\n",
        "                \"top_5_alternatives\": top_5_words_and_probs\n",
        "            })\n",
        "            cnt = 0\n",
        "            while cnt<3:\n",
        "                if predicted_id == word_to_idx[' ']:\n",
        "                    break\n",
        "                if predicted_id in generated_ids:\n",
        "                    x = random.randint(0, 4)\n",
        "                    predicted_id = top_5_indices[x].item()\n",
        "                    cnt+=1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            generated_ids.append(predicted_id)\n",
        "\n",
        "            current_input_ids.append(predicted_id)\n",
        "\n",
        "            if len(current_input_ids) > max_sequence_length:\n",
        "                current_input_ids = current_input_ids[1:]\n",
        "\n",
        "            input_tensor = torch.tensor([current_input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    generated_text = indices_to_text(generated_ids, idx_to_word)\n",
        "    if top5 == True:\n",
        "      print(\"\\n--- Prediction Details ---\")\n",
        "      for detail in prediction_details:\n",
        "          print(f\"Step {detail['step']}:'\")\n",
        "          print(f\"  Top 5 likely words: {', '.join(detail['top_5_alternatives'])}\")\n",
        "      print(\"--------------------------\\n\")\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "5Nvo9n4ztCYX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load data and embeddings from pickle file\n",
        "with open(\"embeddings.pkl\", 'rb') as f:\n",
        "    embeddings = pickle.load(f)\n",
        "with open(\"data.pkl\", 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "train_sequences = data['train_sequences']\n",
        "val_sequences = data['val_sequences']\n",
        "test_sequences = data['test_sequences']\n",
        "\n",
        "word_to_idx = embeddings['word_to_idx']\n",
        "idx_to_word = embeddings['idx_to_word']\n",
        "embeddings_matrix = embeddings['embeddings_matrix']\n",
        "\n",
        "# 2. Initialize parameters\n",
        "pad_token_id = word_to_idx[\"<PAD>\"]\n",
        "vocab_size = len(word_to_idx)\n",
        "embedding_dim = embeddings_matrix.shape[1]\n",
        "batch_size = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 3. Prepare train, val, test data loaders\n",
        "X_train, y_train = create_sequences_and_targets(train_sequences)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "X_val, y_val = create_sequences_and_targets(val_sequences)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "X_test, y_test = create_sequences_and_targets(test_sequences)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#4. Initialize, train and evaluate model\n",
        "model = NextWordPredictor(vocab_size, embeddings_matrix, hidden_dim=256, num_layers=2, pad_token_id=pad_token_id)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "hDcVJI-jj7CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_dataloader, val_dataloader, num_epochs=125, vocab_size=vocab_size, device=device)\n",
        "evaluate_model(model, test_dataloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsiEfiU2uuS4",
        "outputId": "00f2d5ad-e1c7-4cfb-8d1b-c37037e54634"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch [1/125] Train Loss: 4.1127, Validation Loss: 3.3778\n",
            "Epoch [2/125] Train Loss: 3.1283, Validation Loss: 3.0681\n",
            "Epoch [3/125] Train Loss: 2.9512, Validation Loss: 2.9104\n",
            "Epoch [4/125] Train Loss: 2.8004, Validation Loss: 2.7979\n",
            "Epoch [5/125] Train Loss: 2.6838, Validation Loss: 2.7084\n",
            "Epoch [6/125] Train Loss: 2.5883, Validation Loss: 2.6343\n",
            "Epoch [7/125] Train Loss: 2.5037, Validation Loss: 2.5717\n",
            "Epoch [8/125] Train Loss: 2.4266, Validation Loss: 2.5151\n",
            "Epoch [9/125] Train Loss: 2.3559, Validation Loss: 2.4644\n",
            "Calculating metrics...\n",
            "Epoch [10/125]:\n",
            "  Train Loss: 2.2894, Val Loss: 2.4163\n",
            "  Train Accuracy: 0.5894, Val Accuracy: 0.5803\n",
            "  Train Perplexity: 9.46, Val Perplexity: 11.20\n",
            "------------------------------------------------------------\n",
            "Epoch [11/125] Train Loss: 2.2257, Validation Loss: 2.3714\n",
            "Epoch [12/125] Train Loss: 2.1647, Validation Loss: 2.3279\n",
            "Epoch [13/125] Train Loss: 2.1071, Validation Loss: 2.2903\n",
            "Epoch [14/125] Train Loss: 2.0533, Validation Loss: 2.2554\n",
            "Epoch [15/125] Train Loss: 2.0021, Validation Loss: 2.2234\n",
            "Epoch [16/125] Train Loss: 1.9541, Validation Loss: 2.1943\n",
            "Epoch [17/125] Train Loss: 1.9084, Validation Loss: 2.1679\n",
            "Epoch [18/125] Train Loss: 1.8649, Validation Loss: 2.1417\n",
            "Epoch [19/125] Train Loss: 1.8233, Validation Loss: 2.1186\n",
            "Calculating metrics...\n",
            "Epoch [20/125]:\n",
            "  Train Loss: 1.7833, Val Loss: 2.0965\n",
            "  Train Accuracy: 0.6543, Val Accuracy: 0.6295\n",
            "  Train Perplexity: 5.73, Val Perplexity: 8.14\n",
            "------------------------------------------------------------\n",
            "Epoch [21/125] Train Loss: 1.7445, Validation Loss: 2.0747\n",
            "Epoch [22/125] Train Loss: 1.7072, Validation Loss: 2.0537\n",
            "Epoch [23/125] Train Loss: 1.6709, Validation Loss: 2.0354\n",
            "Epoch [24/125] Train Loss: 1.6355, Validation Loss: 2.0157\n",
            "Epoch [25/125] Train Loss: 1.6009, Validation Loss: 1.9972\n",
            "Epoch [26/125] Train Loss: 1.5672, Validation Loss: 1.9784\n",
            "Epoch [27/125] Train Loss: 1.5341, Validation Loss: 1.9608\n",
            "Epoch [28/125] Train Loss: 1.5021, Validation Loss: 1.9443\n",
            "Epoch [29/125] Train Loss: 1.4708, Validation Loss: 1.9263\n",
            "Calculating metrics...\n",
            "Epoch [30/125]:\n",
            "  Train Loss: 1.4400, Val Loss: 1.9095\n",
            "  Train Accuracy: 0.7116, Val Accuracy: 0.6598\n",
            "  Train Perplexity: 4.04, Val Perplexity: 6.75\n",
            "------------------------------------------------------------\n",
            "Epoch [31/125] Train Loss: 1.4098, Validation Loss: 1.8941\n",
            "Epoch [32/125] Train Loss: 1.3802, Validation Loss: 1.8780\n",
            "Epoch [33/125] Train Loss: 1.3510, Validation Loss: 1.8621\n",
            "Epoch [34/125] Train Loss: 1.3228, Validation Loss: 1.8479\n",
            "Epoch [35/125] Train Loss: 1.2951, Validation Loss: 1.8326\n",
            "Epoch [36/125] Train Loss: 1.2681, Validation Loss: 1.8184\n",
            "Epoch [37/125] Train Loss: 1.2416, Validation Loss: 1.8048\n",
            "Epoch [38/125] Train Loss: 1.2159, Validation Loss: 1.7909\n",
            "Epoch [39/125] Train Loss: 1.1904, Validation Loss: 1.7784\n",
            "Calculating metrics...\n",
            "Epoch [40/125]:\n",
            "  Train Loss: 1.1658, Val Loss: 1.7647\n",
            "  Train Accuracy: 0.7630, Val Accuracy: 0.6847\n",
            "  Train Perplexity: 3.06, Val Perplexity: 5.84\n",
            "------------------------------------------------------------\n",
            "Epoch [41/125] Train Loss: 1.1415, Validation Loss: 1.7511\n",
            "Epoch [42/125] Train Loss: 1.1178, Validation Loss: 1.7392\n",
            "Epoch [43/125] Train Loss: 1.0947, Validation Loss: 1.7261\n",
            "Epoch [44/125] Train Loss: 1.0722, Validation Loss: 1.7136\n",
            "Epoch [45/125] Train Loss: 1.0501, Validation Loss: 1.7023\n",
            "Epoch [46/125] Train Loss: 1.0283, Validation Loss: 1.6903\n",
            "Epoch [47/125] Train Loss: 1.0073, Validation Loss: 1.6785\n",
            "Epoch [48/125] Train Loss: 0.9866, Validation Loss: 1.6676\n",
            "Epoch [49/125] Train Loss: 0.9661, Validation Loss: 1.6574\n",
            "Calculating metrics...\n",
            "Epoch [50/125]:\n",
            "  Train Loss: 0.9461, Val Loss: 1.6465\n",
            "  Train Accuracy: 0.8077, Val Accuracy: 0.7064\n",
            "  Train Perplexity: 2.45, Val Perplexity: 5.19\n",
            "------------------------------------------------------------\n",
            "Epoch [51/125] Train Loss: 0.9268, Validation Loss: 1.6345\n",
            "Epoch [52/125] Train Loss: 0.9081, Validation Loss: 1.6246\n",
            "Epoch [53/125] Train Loss: 0.8893, Validation Loss: 1.6148\n",
            "Epoch [54/125] Train Loss: 0.8713, Validation Loss: 1.6032\n",
            "Epoch [55/125] Train Loss: 0.8534, Validation Loss: 1.5946\n",
            "Epoch [56/125] Train Loss: 0.8363, Validation Loss: 1.5850\n",
            "Epoch [57/125] Train Loss: 0.8194, Validation Loss: 1.5747\n",
            "Epoch [58/125] Train Loss: 0.8027, Validation Loss: 1.5656\n",
            "Epoch [59/125] Train Loss: 0.7866, Validation Loss: 1.5566\n",
            "Calculating metrics...\n",
            "Epoch [60/125]:\n",
            "  Train Loss: 0.7707, Val Loss: 1.5476\n",
            "  Train Accuracy: 0.8457, Val Accuracy: 0.7260\n",
            "  Train Perplexity: 2.05, Val Perplexity: 4.70\n",
            "------------------------------------------------------------\n",
            "Epoch [61/125] Train Loss: 0.7553, Validation Loss: 1.5388\n",
            "Epoch [62/125] Train Loss: 0.7402, Validation Loss: 1.5298\n",
            "Epoch [63/125] Train Loss: 0.7255, Validation Loss: 1.5225\n",
            "Epoch [64/125] Train Loss: 0.7112, Validation Loss: 1.5132\n",
            "Epoch [65/125] Train Loss: 0.6969, Validation Loss: 1.5044\n",
            "Epoch [66/125] Train Loss: 0.6828, Validation Loss: 1.4979\n",
            "Epoch [67/125] Train Loss: 0.6696, Validation Loss: 1.4893\n",
            "Epoch [68/125] Train Loss: 0.6566, Validation Loss: 1.4823\n",
            "Epoch [69/125] Train Loss: 0.6437, Validation Loss: 1.4749\n",
            "Calculating metrics...\n",
            "Epoch [70/125]:\n",
            "  Train Loss: 0.6312, Val Loss: 1.4659\n",
            "  Train Accuracy: 0.8772, Val Accuracy: 0.7431\n",
            "  Train Perplexity: 1.78, Val Perplexity: 4.33\n",
            "------------------------------------------------------------\n",
            "Epoch [71/125] Train Loss: 0.6187, Validation Loss: 1.4597\n",
            "Epoch [72/125] Train Loss: 0.6068, Validation Loss: 1.4518\n",
            "Epoch [73/125] Train Loss: 0.5954, Validation Loss: 1.4455\n",
            "Epoch [74/125] Train Loss: 0.5840, Validation Loss: 1.4391\n",
            "Epoch [75/125] Train Loss: 0.5727, Validation Loss: 1.4324\n",
            "Epoch [76/125] Train Loss: 0.5619, Validation Loss: 1.4270\n",
            "Epoch [77/125] Train Loss: 0.5510, Validation Loss: 1.4193\n",
            "Epoch [78/125] Train Loss: 0.5407, Validation Loss: 1.4143\n",
            "Epoch [79/125] Train Loss: 0.5307, Validation Loss: 1.4072\n",
            "Calculating metrics...\n",
            "Epoch [80/125]:\n",
            "  Train Loss: 0.5208, Val Loss: 1.4022\n",
            "  Train Accuracy: 0.9012, Val Accuracy: 0.7571\n",
            "  Train Perplexity: 1.59, Val Perplexity: 4.06\n",
            "------------------------------------------------------------\n",
            "Epoch [81/125] Train Loss: 0.5110, Validation Loss: 1.3958\n",
            "Epoch [82/125] Train Loss: 0.5016, Validation Loss: 1.3894\n",
            "Epoch [83/125] Train Loss: 0.4923, Validation Loss: 1.3862\n",
            "Epoch [84/125] Train Loss: 0.4831, Validation Loss: 1.3796\n",
            "Epoch [85/125] Train Loss: 0.4743, Validation Loss: 1.3744\n",
            "Epoch [86/125] Train Loss: 0.4656, Validation Loss: 1.3695\n",
            "Epoch [87/125] Train Loss: 0.4573, Validation Loss: 1.3645\n",
            "Epoch [88/125] Train Loss: 0.4492, Validation Loss: 1.3594\n",
            "Epoch [89/125] Train Loss: 0.4410, Validation Loss: 1.3523\n",
            "Calculating metrics...\n",
            "Epoch [90/125]:\n",
            "  Train Loss: 0.4332, Val Loss: 1.3493\n",
            "  Train Accuracy: 0.9214, Val Accuracy: 0.7698\n",
            "  Train Perplexity: 1.46, Val Perplexity: 3.85\n",
            "------------------------------------------------------------\n",
            "Epoch [91/125] Train Loss: 0.4254, Validation Loss: 1.3436\n",
            "Epoch [92/125] Train Loss: 0.4179, Validation Loss: 1.3401\n",
            "Epoch [93/125] Train Loss: 0.4104, Validation Loss: 1.3355\n",
            "Epoch [94/125] Train Loss: 0.4034, Validation Loss: 1.3318\n",
            "Epoch [95/125] Train Loss: 0.3963, Validation Loss: 1.3274\n",
            "Epoch [96/125] Train Loss: 0.3893, Validation Loss: 1.3220\n",
            "Epoch [97/125] Train Loss: 0.3826, Validation Loss: 1.3188\n",
            "Epoch [98/125] Train Loss: 0.3760, Validation Loss: 1.3145\n",
            "Epoch [99/125] Train Loss: 0.3696, Validation Loss: 1.3100\n",
            "Calculating metrics...\n",
            "Epoch [100/125]:\n",
            "  Train Loss: 0.3633, Val Loss: 1.3079\n",
            "  Train Accuracy: 0.9374, Val Accuracy: 0.7796\n",
            "  Train Perplexity: 1.36, Val Perplexity: 3.69\n",
            "------------------------------------------------------------\n",
            "Epoch [101/125] Train Loss: 0.3569, Validation Loss: 1.3040\n",
            "Epoch [102/125] Train Loss: 0.3509, Validation Loss: 1.3000\n",
            "Epoch [103/125] Train Loss: 0.3449, Validation Loss: 1.2956\n",
            "Epoch [104/125] Train Loss: 0.3393, Validation Loss: 1.2946\n",
            "Epoch [105/125] Train Loss: 0.3335, Validation Loss: 1.2882\n",
            "Epoch [106/125] Train Loss: 0.3284, Validation Loss: 1.2861\n",
            "Epoch [107/125] Train Loss: 0.3228, Validation Loss: 1.2832\n",
            "Epoch [108/125] Train Loss: 0.3175, Validation Loss: 1.2793\n",
            "Epoch [109/125] Train Loss: 0.3124, Validation Loss: 1.2764\n",
            "Calculating metrics...\n",
            "Epoch [110/125]:\n",
            "  Train Loss: 0.3075, Val Loss: 1.2737\n",
            "  Train Accuracy: 0.9497, Val Accuracy: 0.7888\n",
            "  Train Perplexity: 1.29, Val Perplexity: 3.57\n",
            "------------------------------------------------------------\n",
            "Epoch [111/125] Train Loss: 0.3024, Validation Loss: 1.2714\n",
            "Epoch [112/125] Train Loss: 0.2975, Validation Loss: 1.2692\n",
            "Epoch [113/125] Train Loss: 0.2928, Validation Loss: 1.2658\n",
            "Epoch [114/125] Train Loss: 0.2880, Validation Loss: 1.2637\n",
            "Epoch [115/125] Train Loss: 0.2837, Validation Loss: 1.2596\n",
            "Epoch [116/125] Train Loss: 0.2792, Validation Loss: 1.2585\n",
            "Epoch [117/125] Train Loss: 0.2748, Validation Loss: 1.2576\n",
            "Epoch [118/125] Train Loss: 0.2705, Validation Loss: 1.2527\n",
            "Epoch [119/125] Train Loss: 0.2663, Validation Loss: 1.2505\n",
            "Calculating metrics...\n",
            "Epoch [120/125]:\n",
            "  Train Loss: 0.2623, Val Loss: 1.2494\n",
            "  Train Accuracy: 0.9592, Val Accuracy: 0.7956\n",
            "  Train Perplexity: 1.23, Val Perplexity: 3.48\n",
            "------------------------------------------------------------\n",
            "Epoch [121/125] Train Loss: 0.2582, Validation Loss: 1.2461\n",
            "Epoch [122/125] Train Loss: 0.2543, Validation Loss: 1.2435\n",
            "Epoch [123/125] Train Loss: 0.2506, Validation Loss: 1.2407\n",
            "Epoch [124/125] Train Loss: 0.2467, Validation Loss: 1.2393\n",
            "Epoch [125/125] Train Loss: 0.2429, Validation Loss: 1.2366\n",
            "Training finished.\n",
            "Evaluating model on test set...\n",
            "Final Test Results:\n",
            "  Test Accuracy: 0.8031\n",
            "  Test Perplexity: 3.34\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8030620467365028, 3.340053791542031)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_seed_text = [\"Boots which extended halfway\", \"I ordered him to pay\", \"I answered that it had\", \"remove crusted mud from it. Hence\", \"He never spoke of the\"]\n",
        "num_words = [50, 50, 50, 50, 10]\n",
        "top_5 = [False, False, False, False, True]\n",
        "for i in range(len(start_seed_text)):\n",
        "    print(f\"\\n--- Generating text for seed: '{start_seed_text[i]}' ---\")\n",
        "    generated_output = generate_text(model, word_to_idx, idx_to_word, start_seed_text[i], num_words[i], top5=top_5[i], temperature= 0.5, max_sequence_length=30, device=device)\n",
        "    print(f\"Generated text: {generated_output}\")\n",
        "    print(\"----------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZvZkBbKCn-Z",
        "outputId": "9f5b16ea-d5f6-456f-d217-d621639ee74b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating text for seed: 'Boots which extended halfway' ---\n",
            "Generating from: 'Boots which extended halfway'\n",
            "Generated text: boots which extended halfway over all that press and held a pounds work. what took place my earth, said he and! a secret was in his\n",
            "----------------------\n",
            "\n",
            "--- Generating text for seed: 'I ordered him to pay' ---\n",
            "Generating from: 'I ordered him to pay'\n",
            "Generated text: i ordered him to pay over fifty pounds 1000 work. he was on professional business in discovering half, for a week after with his dreams and land when\n",
            "----------------------\n",
            "\n",
            "--- Generating text for seed: 'I answered that it had' ---\n",
            "Generating from: 'I answered that it had'\n",
            "Generated text: i answered that it had been conviction if she carried out from his death but taken up by history and never mind to get for his work in my\n",
            "----------------------\n",
            "\n",
            "--- Generating text for seed: 'remove crusted mud from it. Hence' ---\n",
            "Generating from: 'remove crusted mud from it. Hence'\n",
            "Generated text: remove mud from it. hence ? well, how took to doctors round it, and so! what do my business knew how give me to buy his mother!\n",
            "----------------------\n",
            "\n",
            "--- Generating text for seed: 'He never spoke of the' ---\n",
            "Generating from: 'He never spoke of the'\n",
            "\n",
            "--- Prediction Details ---\n",
            "Step 1:'\n",
            "  Top 5 likely words:   (1.0000), - (0.0000), silence (0.0000), unpleasant (0.0000), ! (0.0000)\n",
            "Step 2:'\n",
            "  Top 5 likely words: bridegroom (0.3350), manner (0.2153), words (0.1213), death (0.0664), silence (0.0493)\n",
            "Step 3:'\n",
            "  Top 5 likely words:   (0.9917), , (0.0055), . (0.0016), death (0.0009), ! (0.0002)\n",
            "Step 4:'\n",
            "  Top 5 likely words: himself (0.6891), never (0.2624), of (0.0239), which (0.0107), before (0.0034)\n",
            "Step 5:'\n",
            "  Top 5 likely words:   (0.9976), , (0.0023), ? (0.0000), ! (0.0000), . (0.0000)\n",
            "Step 6:'\n",
            "  Top 5 likely words: never (0.4794), he (0.2339), himself (0.1464), it (0.0485), him (0.0427)\n",
            "Step 7:'\n",
            "  Top 5 likely words:   (0.9796), . (0.0174), , (0.0019), ? (0.0012), ! (0.0000)\n",
            "Step 8:'\n",
            "  Top 5 likely words: of (0.6853), for (0.0783), also (0.0774), in (0.0452), never (0.0243)\n",
            "Step 9:'\n",
            "  Top 5 likely words:   (1.0000), - (0.0000), ? (0.0000), , (0.0000), . (0.0000)\n",
            "Step 10:'\n",
            "  Top 5 likely words: life (0.9782), strange (0.0063), himself (0.0051), <UNK> (0.0025), course (0.0015)\n",
            "--------------------------\n",
            "\n",
            "Generated text: he never spoke of the manner before him of life\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XW0G0r-3CsY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}